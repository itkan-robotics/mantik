{
    "title": "Vision-Based Pose Estimation",
    "sections": [
        {
            "type": "text",
            "title": "Overview",
            "content": "Vision-based pose estimation uses cameras to observe the environment and calculate the robot's position. In FRC, this primarily relies on <strong>AprilTags</strong>. Unlike odometry, which tracks relative movement, vision provides absolute coordinates on the field. This allows a robot to \"localize\" itself even after wheel slip or collisions."
        },
        {
            "type": "text",
            "title": "How It Works",
            "content": "1. <strong>Detection:</strong> The camera identifies an AprilTag and its ID.<br>2. <strong>Lookup:</strong> The system looks up the tag's known field position (X, Y, Z, Rotation) from a standard field layout map.<br>3. <strong>Solve PnP:</strong> Using the tag's apparent size and orientation in the camera frame, the system calculates the camera's position relative to the tag.<br>4. <strong>Transformation:</strong> The system applies the camera-to-robot offset to find the robot's position on the field."
        },
        {
            "type": "text",
            "title": "Implementation Approaches",
            "content": "You can implement vision pose estimation using specific libraries that handle the complex math:<br><br><strong>PhotonLib (PhotonVision):</strong> Uses the <code>PhotonPoseEstimator</code> class. It takes a field layout, camera properties, and pipeline results to output a robot pose.<br><br><strong>LimelightHelpers:</strong> Limelight computes the pose on-board. You simply retrieve the <code>BotPose</code> array from NetworkTables (or use the helper methods) to get the calculated field position."
        },
        {
            "type": "code-tabs",
            "title": "Example: PhotonPoseEstimator",
            "description": "Setting up the estimator with PhotonVision. This requires defining the physical transform of the camera relative to the robot center.",
            "tabs": [
                {
                    "label": "Setup",
                    "code": "package frc.robot.subsystems;\n\nimport org.photonvision.PhotonCamera;\nimport org.photonvision.PhotonPoseEstimator;\nimport org.photonvision.PhotonPoseEstimator.PoseStrategy;\nimport edu.wpi.first.apriltag.AprilTagFieldLayout;\nimport edu.wpi.first.apriltag.AprilTagFields;\nimport edu.wpi.first.math.geometry.Transform3d;\nimport edu.wpi.first.math.geometry.Translation3d;\nimport edu.wpi.first.math.geometry.Rotation3d;\n\npublic class VisionSubsystem extends SubsystemBase {\n    private PhotonCamera m_camera = new PhotonCamera(\"MainCamera\");\n    private PhotonPoseEstimator estimator;\n    \n    public VisionSubsystem() {\n        // Define camera position relative to robot center\n        // 0.5m forward, 0.0m left/right, 0.5m up\n        Transform3d robotToCamera = new Transform3d(\n            new Translation3d(0.5, 0.0, 0.5),\n            new Rotation3d(0, 0, 0) // No rotation\n        );\n\n        // Load AprilTag field layout for current game year\n        AprilTagFieldLayout fieldLayout = AprilTagFields.k2024Crescendo.loadAprilTagLayoutField();\n\n        // Create pose estimator\n        estimator = new PhotonPoseEstimator(\n            fieldLayout,\n            PoseStrategy.MULTI_TAG_PNP_ON_COPROCESSOR, // Use multi-tag when available\n            m_camera,\n            robotToCamera\n        );\n    }\n}"
                },
                {
                    "label": "Update Loop",
                    "code": "package frc.robot.subsystems;\n\nimport java.util.Optional;\nimport org.photonvision.EstimatedRobotPose;\nimport org.photonvision.PhotonPoseEstimator;\n\npublic class VisionSubsystem extends SubsystemBase {\n    private PhotonPoseEstimator estimator;\n    \n    /**\n     * Get estimated global pose from vision\n     * @return Optional EstimatedRobotPose if valid, empty otherwise\n     */\n    public Optional<EstimatedRobotPose> getEstimatedGlobalPose() {\n        // Update estimator with latest camera result\n        return estimator.update();\n    }\n}"
                }
            ]
        },
        {
            "type": "text",
            "title": "Handling Ambiguity",
            "content": "Sometimes, a single tag detection can result in two possible valid poses (ambiguity).<br>- <strong>Multi-Tag:</strong> Seeing multiple tags at once eliminates ambiguity and vastly improves accuracy.<br>- <strong>Filters:</strong> If using single tags, reject detections with high ambiguity errors or those that jump significantly far from the robot's current odometry pose."
        },
        {
            "type": "link-grid",
            "title": "Related Topics",
            "links": [
                {
                    "label": "Sensor Fusion",
                    "id": "vision-pose-estimation-fusion"
                },
                {
                    "label": "WPILib AprilTags",
                    "url": "https://docs.wpilib.org/en/stable/docs/software/vision-processing/apriltag/apriltag-intro.html"
                }
            ]
        }
    ]
}