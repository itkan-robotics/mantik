{
    "title": "Vision Subsystem",
    "sections": [
        {
            "type": "text",
            "title": "Why Vision?",
            "content": "While odometry is fast, it drifts over time. Vision systems correct this by detecting fixed field markers, called <strong>AprilTags</strong>. When a camera sees an AprilTag, it can calculate the robot's exact position on the field relative to that tag. This provides an absolute position reference, similar to GPS."
        },
        {
            "type": "text",
            "title": "Hardware Options",
            "content": "Two common vision solutions in FRC are:<br><br><strong>PhotonVision:</strong> Open-source software that runs on coprocessors (like a Raspberry Pi or Orange Pi) or the roboRIO. It supports many camera types and provides a flexible pipeline.<br><br><strong>Limelight:</strong> A dedicated smart camera hardware/software solution. It is easy to set up and processes images directly on the device, sending data to the roboRIO via NetworkTables."
        },
        {
            "type": "text",
            "title": "The Vision Pipeline",
            "content": "1. <strong>Capture:</strong> Camera takes an image.<br>2. <strong>Detect:</strong> Software identifies AprilTags in the image.<br>3. <strong>Solve:</strong> Using the known size of the tag and camera calibration, the system calculates the camera's position relative to the tag.<br>4. <strong>Transform:</strong> The system converts the camera-to-tag position into a robot-to-field position (Pose)."
        },
        {
            "type": "code-tabs",
            "title": "Step 1: Basic Setup",
            "description": "Initialize the vision system. For PhotonVision, this involves a <code>PhotonCamera</code>. For Limelight, you interact with static helper methods or NetworkTables.",
            "tabs": [
                {
                    "label": "PhotonVision",
                    "code": "package frc.robot.subsystems;\n\nimport edu.wpi.first.wpilibj2.command.SubsystemBase;\nimport org.photonvision.PhotonCamera;\n\npublic class VisionSubsystem extends SubsystemBase {\n    // Camera name must match the name configured in PhotonVision UI\n    private final PhotonCamera m_camera;\n    \n    public VisionSubsystem() {\n        // \"MainCamera\" must match the camera name in PhotonVision UI\n        m_camera = new PhotonCamera(\"MainCamera\");\n    }\n}"
                },
                {
                    "label": "Limelight",
                    "code": "package frc.robot.subsystems;\n\nimport edu.wpi.first.wpilibj2.command.SubsystemBase;\nimport frc.robot.LimelightHelpers;\n\npublic class VisionSubsystem extends SubsystemBase {\n    // Camera name (default is \"limelight\")\n    private final String m_cameraName = \"limelight\";\n    \n    public VisionSubsystem() {\n        // Set pipeline to 0 (typically configured for AprilTags)\n        LimelightHelpers.setPipelineIndex(m_cameraName, 0);\n    }\n}"
                }
            ]
        },
        {
            "type": "text",
            "title": "Step 2: Camera-to-Robot Transform",
            "content": "The vision system needs to know where the camera is mounted on the robot. This offset (transform) allows it to translate the camera's position into the robot's center position. Measure the X (forward/back), Y (left/right), and Z (up/down) distances from the robot's center to the camera lens, as well as any rotation."
        },
        {
            "type": "code-tabs",
            "title": "Step 3: Getting Pose Estimates",
            "description": "Retrieve the calculated robot pose from the vision system. We check for validity (e.g., are tags actually visible?) before using the data.",
            "tabs": [
                {
                    "label": "PhotonVision",
                    "code": "package frc.robot.subsystems;\n\nimport java.util.Optional;\nimport edu.wpi.first.math.geometry.Pose2d;\nimport org.photonvision.PhotonCamera;\nimport org.photonvision.targeting.PhotonPipelineResult;\n\npublic class VisionSubsystem extends SubsystemBase {\n    private PhotonCamera m_camera = new PhotonCamera(\"MainCamera\");\n    \n    /**\n     * Get estimated robot pose from vision\n     * Note: For full pose estimation, use PhotonPoseEstimator (see advanced examples)\n     * @return Optional Pose2d if valid target detected, empty otherwise\n     */\n    public Optional<Pose2d> getEstimatedPose() {\n        PhotonPipelineResult result = m_camera.getLatestResult();\n        if (result.hasTargets()) {\n            // Process targets and calculate pose\n            // Full implementation would use PhotonPoseEstimator here\n            return Optional.empty(); // Placeholder\n        }\n        return Optional.empty();\n    }\n}"
                },
                {
                    "label": "Limelight",
                    "code": "package frc.robot.subsystems;\n\nimport java.util.Optional;\nimport edu.wpi.first.math.geometry.Pose2d;\nimport frc.robot.LimelightHelpers;\nimport frc.robot.LimelightHelpers.PoseEstimate;\n\npublic class VisionSubsystem extends SubsystemBase {\n    private final String m_cameraName = \"limelight\";\n    \n    /**\n     * Get estimated robot pose from Limelight\n     * @return Optional Pose2d if valid AprilTag detected, empty otherwise\n     */\n    public Optional<Pose2d> getEstimatedPose() {\n        // Get Blue Alliance coordinate pose\n        // Use getBotPoseEstimate_wpiRed() for Red Alliance\n        PoseEstimate estimate = LimelightHelpers.getBotPoseEstimate_wpiBlue(m_cameraName);\n        \n        // Check if at least one AprilTag was detected\n        if (estimate.tagCount > 0) {\n            return Optional.of(estimate.pose);\n        }\n        return Optional.empty();\n    }\n}"
                }
            ]
        },
        {
            "type": "text",
            "title": "Integration",
            "content": "The data from this subsystem is typically sent to a <code>PoseEstimator</code> class in your Drive subsystem. The Drive subsystem fuses this vision data with odometry to maintain an accurate robot pose. See the <strong>Sensor Fusion</strong> section for details."
        },
        {
            "type": "link-grid",
            "title": "Resources",
            "links": [
                {
                    "label": "Sensor Fusion",
                    "id": "vision-pose-estimation-fusion"
                },
                {
                    "label": "PhotonVision Docs",
                    "url": "https://docs.photonvision.org/en/latest/"
                },
                {
                    "label": "Limelight Docs",
                    "url": "https://docs.limelightvision.io/en/latest/"
                }
            ]
        }
    ]
}