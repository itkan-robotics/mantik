{
    "title": "Vision Subsystem",
    "sections": [
        {
            "type": "text",
            "title": "Why Vision-Based Pose Estimation?",
            "content": "Imagine your robot driving across the field, relying solely on wheel encoders and a gyro. Over time, small errors accumulate—wheel slip, surface variations, measurement inaccuracies. Before you know it, your robot thinks it's in one place, but it's actually several feet away. This is drift, and it's the fundamental challenge of odometry-based localization.<br><br>Vision-based pose estimation solves this problem by providing absolute positioning. Instead of accumulating error over time, your robot can look at the field, see known markers (AprilTags), and instantly know exactly where it is. It's like having GPS for your robot—periodic corrections that eliminate drift and ensure accurate navigation.<br><br>When combined with odometry through sensor fusion, you get the best of both worlds: continuous, fast tracking from odometry, with periodic absolute corrections from vision. This creates a robust, accurate pose estimation system that works reliably in competition conditions."
        },
        {
            "type": "text",
            "title": "Understanding AprilTags",
            "content": "AprilTags are fiducial markers—visual markers with unique black and white patterns that encode a specific ID. Think of them as QR codes designed for robotics. Each AprilTag on an FRC field has a known position and orientation, specified in the official field layout documentation.<br><br>When your camera detects an AprilTag, the vision system can calculate the camera's position relative to that tag. Since the tag's field position is known, and the camera's position relative to your robot is known, the system can determine your robot's exact pose on the field. This happens in real-time, providing instant absolute positioning without any accumulated error.<br><br>FRC fields typically have AprilTags placed at strategic locations: scoring areas, corners, and mid-field positions. Each tag has a unique ID that corresponds to its field position, allowing your robot to identify which tag it's seeing and calculate its pose accordingly."
        },
        {
            "type": "text",
            "title": "The Vision Processing Pipeline",
            "content": "Here's how vision pose estimation works, step by step:<br><br><strong>Step 1: Image Capture</strong> - Your camera continuously captures images of the field at 10-30 frames per second. The camera needs to be properly calibrated to account for lens distortion and measure accurate distances.<br><br><strong>Step 2: Tag Detection</strong> - The vision processor analyzes each image, searching for AprilTag patterns. When a tag is found, the system identifies its unique ID and measures its position in the image.<br><br><strong>Step 3: Pose Calculation</strong> - Using the tag's known size and position in the image, the system calculates the camera's position and orientation relative to the tag. This involves complex geometry and camera calibration data.<br><br><strong>Step 4: Robot Pose Conversion</strong> - The camera pose is converted to robot pose by accounting for the camera's mounting position and orientation relative to the robot center. This gives you the robot's actual position on the field."
        },
        {
            "type": "text",
            "title": "Creating the Vision Subsystem",
            "content": "Now let's create a vision subsystem that can detect AprilTags and provide pose estimates. Both PhotonVision and Limelight are excellent choices for FRC vision processing. PhotonVision is an open-source solution that runs on coprocessors, while Limelight is a dedicated smart camera. Both provide similar functionality for AprilTag detection and robot localization.<br><br>For PhotonVision, you'll configure your camera in the PhotonVision UI and use the PhotonCamera class in your code. For Limelight, you'll use the LimelightHelpers utility class which simplifies accessing Limelight's NetworkTables data. Both systems can provide pose estimates that work seamlessly with WPILib's PoseEstimator."
        },
        {
            "type": "code-tabs",
            "title": "Step 1: Creating the Camera Object",
            "description": "The first step is to create a camera object that connects to your vision system. For PhotonVision, you create a PhotonCamera object with the camera name that matches your PhotonVision configuration. For Limelight, you don't need to create a camera object—instead, you'll use the LimelightHelpers class with your camera's name (or \"limelight\" for the default camera).",
            "tabs": [
                {
                    "label": "PhotonVision",
                    "code": "import org.photonvision.PhotonCamera;\n\npublic class VisionSubsystem {\n    private final PhotonCamera m_camera;\n    \n    public VisionSubsystem() {\n        // Camera name must match the name configured in PhotonVision UI\n        m_camera = new PhotonCamera(\"MainCamera\");\n    }\n}"
                },
                {
                    "label": "Limelight",
                    "code": "import frc.robot.LimelightHelpers;\n\npublic class VisionSubsystem {\n    private final String m_limelightName;\n    private final int m_pipelineIndex;\n    \n    public VisionSubsystem() {\n        this(\"limelight\", 0);\n    }\n    \n    public VisionSubsystem(String limelightName, int pipelineIndex) {\n        m_limelightName = limelightName;\n        m_pipelineIndex = pipelineIndex;\n        \n        // Configure Limelight pipeline\n        LimelightHelpers.setPipelineIndex(m_limelightName, m_pipelineIndex);\n    }\n}"
                }
            ]
        },
        {
            "type": "code-tabs",
            "title": "Step 2: Configuring Camera Pose and PhotonPoseEstimator",
            "description": "Your camera needs to know its position and orientation relative to your robot's center. For PhotonVision, you also need to create a PhotonPoseEstimator which handles pose calculation from AprilTag detections. The estimator uses the field layout (AprilTag positions) and camera pose configuration. For Limelight, you configure the pipeline index and set the camera pose programmatically using setPipelineIndex() and setCameraPose_RobotSpace().",
            "tabs": [
                {
                    "label": "PhotonVision",
                    "code": "import edu.wpi.first.math.geometry.Transform3d;\nimport edu.wpi.first.apriltag.AprilTagFieldLayout;\nimport org.photonvision.PhotonPoseEstimator;\nimport org.photonvision.PhotonPoseEstimator.PoseStrategy;\n\npublic class VisionSubsystem {\n    // Camera position relative to robot center (in meters)\n    // Configure this in PhotonVision UI under \"Camera Settings\" -> \"Robot to Camera Transform\"\n    // Or load from code if using WPILib's AprilTagFieldLayout\n    private static final Transform3d kRobotToCamera = new Transform3d(\n        0.3, 0.0, 0.2,  // X, Y, Z position (meters)\n        new edu.wpi.first.math.geometry.Rotation3d()  // Rotation\n    );\n    \n    // Field layout with AprilTag positions\n    // Load from WPILib's AprilTagFieldLayout or configure in PhotonVision UI\n    private final AprilTagFieldLayout kTagLayout;\n    \n    private final PhotonPoseEstimator m_photonEstimator;\n    \n    public VisionSubsystem(AprilTagFieldLayout tagLayout) {\n        kTagLayout = tagLayout;\n        \n        // Create PhotonPoseEstimator\n        // MULTI_TAG_PNP_ON_COPROCESSOR: Uses multiple tags for better accuracy\n        // Falls back to LOWEST_AMBIGUITY if multi-tag fails\n        m_photonEstimator = new PhotonPoseEstimator(\n            kTagLayout,\n            PoseStrategy.MULTI_TAG_PNP_ON_COPROCESSOR,\n            kRobotToCamera\n        );\n        m_photonEstimator.setMultiTagFallbackStrategy(PoseStrategy.LOWEST_AMBIGUITY);\n    }\n}"
                },
                {
                    "label": "Limelight",
                    "code": "import frc.robot.LimelightHelpers;\n\npublic class VisionSubsystem {\n    private final String m_limelightName;\n    private final int m_pipelineIndex;\n    \n    public VisionSubsystem(String limelightName, int pipelineIndex) {\n        m_limelightName = limelightName;\n        m_pipelineIndex = pipelineIndex;\n        \n        // Configure Limelight pipeline\n        LimelightHelpers.setPipelineIndex(m_limelightName, m_pipelineIndex);\n        \n        // Configure camera pose relative to robot (in meters and degrees)\n        // Parameters: Forward, Side, Up, Roll, Pitch, Yaw\n        LimelightHelpers.setCameraPose_RobotSpace(\n            m_limelightName,\n            0.3,   // Forward (meters) - positive = forward from robot center\n            0.0,   // Side (meters) - positive = left of robot center\n            0.2,   // Up (meters) - positive = above robot center\n            0.0,   // Roll (degrees) - rotation around forward axis\n            0.0,   // Pitch (degrees) - rotation around side axis\n            0.0    // Yaw (degrees) - rotation around up axis\n        );\n    }\n}"
                }
            ]
        },
        {
            "type": "code-tabs",
            "title": "Step 3: Getting AprilTag Detections",
            "description": "Once your camera is configured, you can retrieve AprilTag detections. Both systems provide methods to check if targets are detected and to get information about those targets. PhotonVision uses getAllUnreadResults() to get all unread pipeline results (process each one to avoid missing detections), while Limelight uses getLatestResults() to get JSON-formatted results.",
            "tabs": [
                {
                    "label": "PhotonVision",
                    "code": "import org.photonvision.targeting.PhotonPipelineResult;\nimport org.photonvision.targeting.PhotonTrackedTarget;\nimport java.util.List;\nimport java.util.Optional;\n\npublic class VisionSubsystem {\n    private final PhotonCamera m_camera;\n    \n    public Optional<Integer> getDetectedTagId() {\n        // Get all unread results to ensure we don't miss any detections\n        List<PhotonPipelineResult> results = m_camera.getAllUnreadResults();\n        \n        for (PhotonPipelineResult result : results) {\n            if (result.hasTargets()) {\n                PhotonTrackedTarget bestTarget = result.getBestTarget();\n                int fiducialId = bestTarget.getFiducialId();\n                \n                // Fiducial ID >= 0 means it's an AprilTag\n                if (fiducialId >= 0) {\n                    return Optional.of(fiducialId);\n                }\n            }\n        }\n        \n        return Optional.empty();\n    }\n    \n    public boolean hasTargets() {\n        PhotonPipelineResult result = m_camera.getLatestResult();\n        return result.hasTargets();\n    }\n}"
                },
                {
                    "label": "Limelight",
                    "code": "import frc.robot.LimelightHelpers;\nimport frc.robot.LimelightHelpers.LimelightResults;\nimport java.util.Optional;\n\npublic class VisionSubsystem {\n    private final String m_limelightName;\n    \n    public Optional<Integer> getDetectedTagId() {\n        LimelightResults results = \n            LimelightHelpers.getLatestResults(m_limelightName);\n        \n        // Check if any fiducial targets are detected\n        if (results.valid \n                && results.targets_Fiducials.length > 0) {\n            LimelightHelpers.LimelightTarget_Fiducial target = \n                results.targets_Fiducials[0];\n            \n            return Optional.of((int) target.fiducialID);\n        }\n        \n        return Optional.empty();\n    }\n    \n    public boolean hasTargets() {\n        LimelightResults results = \n            LimelightHelpers.getLatestResults(m_limelightName);\n        return results.valid \n            && results.targets_Fiducials.length > 0;\n    }\n}"
                }
            ]
        },
        {
            "type": "code-tabs",
            "title": "Step 4: Extracting Pose Estimates",
            "description": "The most important part of vision-based pose estimation is extracting the robot's pose from AprilTag detections. PhotonVision uses PhotonPoseEstimator.update() which automatically calculates the robot pose from pipeline results. It returns an EstimatedRobotPose with pose, timestamp, and target information. Limelight provides getBotPoseEstimate_wpiBlue() which returns a PoseEstimate object containing the pose, timestamp, and metadata about the detection quality.",
            "tabs": [
                {
                    "label": "PhotonVision",
                    "code": "import edu.wpi.first.math.geometry.Pose2d;\nimport org.photonvision.EstimatedRobotPose;\nimport org.photonvision.targeting.PhotonPipelineResult;\nimport java.util.Optional;\n\npublic class VisionSubsystem {\n    private final PhotonCamera m_camera;\n    private final PhotonPoseEstimator m_photonEstimator;\n    \n    /**\n     * Process all unread results and get pose estimates.\n     * Returns the latest valid pose estimate if available.\n     */\n    public Optional<Pose2d> getAprilTagPose() {\n        Optional<EstimatedRobotPose> latestEstimate = Optional.empty();\n        \n        // Process all unread results to avoid missing detections\n        for (PhotonPipelineResult result : m_camera.getAllUnreadResults()) {\n            if (result.hasTargets()) {\n                // PhotonPoseEstimator handles pose calculation\n                Optional<EstimatedRobotPose> estimate = m_photonEstimator.update(result);\n                \n                if (estimate.isPresent()) {\n                    latestEstimate = estimate;\n                }\n            }\n        }\n        \n        return latestEstimate.map(est -> est.estimatedPose.toPose2d());\n    }\n    \n    /**\n     * Get pose estimate with timestamp for sensor fusion.\n     * This includes the full EstimatedRobotPose with timestamp and targets.\n     */\n    public Optional<EstimatedRobotPose> getLatestEstimatedPose() {\n        Optional<EstimatedRobotPose> latestEstimate = Optional.empty();\n        \n        for (PhotonPipelineResult result : m_camera.getAllUnreadResults()) {\n            if (result.hasTargets()) {\n                Optional<EstimatedRobotPose> estimate = m_photonEstimator.update(result);\n                if (estimate.isPresent()) {\n                    latestEstimate = estimate;\n                }\n            }\n        }\n        \n        return latestEstimate;\n    }\n}"
                },
                {
                    "label": "Limelight",
                    "code": "import edu.wpi.first.math.geometry.Pose2d;\nimport frc.robot.LimelightHelpers;\nimport frc.robot.LimelightHelpers.PoseEstimate;\nimport java.util.Optional;\n\npublic class VisionSubsystem {\n    private final String m_limelightName;\n    \n    /**\n     * Get pose estimate with full metadata (recommended for sensor fusion)\n     * This includes timestamp, tag count, and quality metrics\n     */\n    public Optional<PoseEstimate> getAprilTagPoseEstimate() {\n        // Get MegaTag pose estimate in WPILib Blue coordinate system\n        // For 2024+, always use wpiBlue regardless of alliance\n        PoseEstimate poseEstimate = \n            LimelightHelpers.getBotPoseEstimate_wpiBlue(m_limelightName);\n        \n        // Validate the pose estimate\n        // Checks that tags were detected and pose is valid\n        if (LimelightHelpers.validPoseEstimate(poseEstimate)) {\n            return Optional.of(poseEstimate);\n        }\n        \n        return Optional.empty();\n    }\n    \n    /**\n     * Get just the pose (2D position and heading)\n     * Use this if you only need the pose without metadata\n     */\n    public Optional<Pose2d> getAprilTagPose() {\n        Optional<PoseEstimate> estimate = getAprilTagPoseEstimate();\n        \n        if (estimate.isPresent()) {\n            return Optional.of(estimate.get().pose);\n        }\n        \n        return Optional.empty();\n    }\n    \n    /**\n     * Check if a valid pose estimate is available\n     */\n    public boolean hasValidPoseEstimate() {\n        PoseEstimate estimate = \n            LimelightHelpers.getBotPoseEstimate_wpiBlue(m_limelightName);\n        return LimelightHelpers.validPoseEstimate(estimate);\n    }\n}"
                }
            ]
        },
        {
            "type": "code-tabs",
            "title": "Complete Vision Subsystem",
            "content": "Here's the complete vision subsystem implementation using PhotonPoseEstimator. This follows the production-ready pattern from PhotonVision examples. The EstimateConsumer interface allows you to pass pose estimates directly to your SwerveDrivePoseEstimator with dynamic standard deviations.",
            "tabs": [
                {
                    "label": "PhotonVision",
                    "code": "package frc.robot.subsystems;\n\nimport edu.wpi.first.math.Matrix;\nimport edu.wpi.first.math.VecBuilder;\nimport edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.math.geometry.Transform3d;\nimport edu.wpi.first.math.numbers.N1;\nimport edu.wpi.first.math.numbers.N3;\nimport edu.wpi.first.apriltag.AprilTagFieldLayout;\nimport edu.wpi.first.wpilibj2.command.SubsystemBase;\nimport java.util.List;\nimport java.util.Optional;\nimport org.photonvision.EstimatedRobotPose;\nimport org.photonvision.PhotonCamera;\nimport org.photonvision.PhotonPoseEstimator;\nimport org.photonvision.PhotonPoseEstimator.PoseStrategy;\nimport org.photonvision.targeting.PhotonPipelineResult;\nimport org.photonvision.targeting.PhotonTrackedTarget;\n\n/**\n * Vision subsystem for AprilTag detection using PhotonVision.\n * \n * This implementation uses PhotonPoseEstimator which provides:\n * - Automatic pose calculation from AprilTag detections\n * - Support for multi-tag pose estimation (more accurate)\n * - Fallback strategies for single-tag scenarios\n * - Proper timestamp handling for sensor fusion\n * \n * IMPORTANT: Configure the following:\n * 1. Camera name must match the camera name passed to constructor\n * 2. Set up AprilTag pipeline in PhotonVision UI\n * 3. Load AprilTagFieldLayout (from WPILib or PhotonVision UI)\n * 4. Configure Robot to Camera Transform (in UI or via code)\n */\npublic class VisionSubsystem extends SubsystemBase {\n    private final PhotonCamera m_camera;\n    private final PhotonPoseEstimator m_photonEstimator;\n    private Matrix<N3, N1> m_currentStdDevs;\n    private final EstimateConsumer m_estimateConsumer;\n    \n    // Standard deviation constants for pose estimation\n    // Lower values = more trust in the measurement\n    private static final Matrix<N3, N1> kSingleTagStdDevs = \n        VecBuilder.fill(0.7, 0.7, Double.MAX_VALUE);  // X, Y, heading (meters, meters, radians)\n    private static final Matrix<N3, N1> kMultiTagStdDevs = \n        VecBuilder.fill(0.5, 0.5, Double.MAX_VALUE);  // Better accuracy with multiple tags\n    \n    /**\n     * Functional interface for consuming pose estimates.\n     * Pass this to your SwerveDrivePoseEstimator.addVisionMeasurement()\n     */\n    @FunctionalInterface\n    public interface EstimateConsumer {\n        void accept(Pose2d pose, double timestamp, Matrix<N3, N1> stdDevs);\n    }\n    \n    /**\n     * Create a new VisionSubsystem.\n     * \n     * @param cameraName Name of the camera configured in PhotonVision UI\n     * @param tagLayout AprilTag field layout with tag positions\n     * @param robotToCamera Transform from robot center to camera\n     * @param estimateConsumer Lambda to pass estimates to your pose estimator\n     */\n    public VisionSubsystem(\n            String cameraName,\n            AprilTagFieldLayout tagLayout,\n            Transform3d robotToCamera,\n            EstimateConsumer estimateConsumer) {\n        \n        m_camera = new PhotonCamera(cameraName);\n        m_estimateConsumer = estimateConsumer;\n        \n        // Create PhotonPoseEstimator\n        // MULTI_TAG_PNP_ON_COPROCESSOR: Uses multiple tags for better accuracy\n        // Falls back to LOWEST_AMBIGUITY if multi-tag estimation fails\n        m_photonEstimator = new PhotonPoseEstimator(\n            tagLayout,\n            PoseStrategy.MULTI_TAG_PNP_ON_COPROCESSOR,\n            robotToCamera\n        );\n        m_photonEstimator.setMultiTagFallbackStrategy(PoseStrategy.LOWEST_AMBIGUITY);\n        \n        // Initialize with single-tag standard deviations\n        m_currentStdDevs = kSingleTagStdDevs;\n    }\n    \n    @Override\n    public void periodic() {\n        // Process all unread results to avoid missing detections\n        for (PhotonPipelineResult result : m_camera.getAllUnreadResults()) {\n            if (result.hasTargets()) {\n                // Get pose estimate from PhotonPoseEstimator\n                Optional<EstimatedRobotPose> visionEst = m_photonEstimator.update(result);\n                \n                // Calculate dynamic standard deviations based on tag count and distance\n                updateEstimationStdDevs(visionEst, result.getTargets());\n                \n                // Pass estimate to pose estimator with calculated standard deviations\n                visionEst.ifPresent(est -> {\n                    m_estimateConsumer.accept(\n                        est.estimatedPose.toPose2d(),\n                        est.timestampSeconds,\n                        m_currentStdDevs\n                    );\n                });\n            }\n        }\n    }\n    \n    /**\n     * Calculate dynamic standard deviations based on tag count and distance.\n     * This heuristic adjusts trust in measurements:\n     * - Multiple tags = more accurate = lower std devs\n     * - Single tag far away = less accurate = higher std devs\n     * \n     * @param estimatedPose The estimated pose (may be empty)\n     * @param targets All targets in this camera frame\n     */\n    private void updateEstimationStdDevs(\n            Optional<EstimatedRobotPose> estimatedPose,\n            List<PhotonTrackedTarget> targets) {\n        \n        if (estimatedPose.isEmpty()) {\n            // No pose input, default to single-tag std devs\n            m_currentStdDevs = kSingleTagStdDevs;\n            return;\n        }\n        \n        // Count AprilTags and calculate average distance\n        int numTags = 0;\n        double avgDist = 0;\n        \n        for (PhotonTrackedTarget target : targets) {\n            if (target.getFiducialId() < 0) continue;  // Skip non-AprilTag targets\n            \n            // Get tag pose from field layout\n            var tagPose = m_photonEstimator.getFieldTags()\n                .getTagPose(target.getFiducialId());\n            \n            if (tagPose.isEmpty()) continue;\n            \n            numTags++;\n            \n            // Calculate distance from robot to tag\n            double distance = tagPose.get().toPose2d().getTranslation()\n                .getDistance(estimatedPose.get().estimatedPose.toPose2d().getTranslation());\n            \n            avgDist += distance;\n        }\n        \n        if (numTags == 0) {\n            // No tags visible, default to single-tag std devs\n            m_currentStdDevs = kSingleTagStdDevs;\n        } else {\n            // Calculate average distance\n            avgDist /= numTags;\n            \n            // Start with base std devs\n            Matrix<N3, N1> estStdDevs;\n            if (numTags > 1) {\n                // Multiple tags = more accurate\n                estStdDevs = kMultiTagStdDevs;\n            } else {\n                // Single tag\n                estStdDevs = kSingleTagStdDevs;\n                \n                // Reject single tags that are too far away\n                if (avgDist > 4.0) {  // 4 meters\n                    estStdDevs = VecBuilder.fill(\n                        Double.MAX_VALUE,\n                        Double.MAX_VALUE,\n                        Double.MAX_VALUE\n                    );\n                }\n            }\n            \n            // Increase std devs based on distance (farther = less accurate)\n            // Formula: stdDev *= (1 + distance^2 / 30)\n            if (estStdDevs.get(0, 0) != Double.MAX_VALUE) {\n                estStdDevs = estStdDevs.times(1 + (avgDist * avgDist / 30.0));\n            }\n            \n            m_currentStdDevs = estStdDevs;\n        }\n    }\n    \n    /**\n     * Get the current standard deviations for pose estimation.\n     * \n     * @return Current standard deviations matrix\n     */\n    public Matrix<N3, N1> getEstimationStdDevs() {\n        return m_currentStdDevs;\n    }\n    \n    /**\n     * Get robot pose estimate from latest detection.\n     * \n     * @return Optional containing pose estimate if available\n     */\n    public Optional<Pose2d> getAprilTagPose() {\n        PhotonPipelineResult result = m_camera.getLatestResult();\n        \n        if (result.hasTargets()) {\n            Optional<EstimatedRobotPose> estimate = m_photonEstimator.update(result);\n            return estimate.map(est -> est.estimatedPose.toPose2d());\n        }\n        \n        return Optional.empty();\n    }\n    \n    /**\n     * Get the ID of the detected AprilTag.\n     * \n     * @return Optional containing tag ID if detected\n     */\n    public Optional<Integer> getDetectedTagId() {\n        PhotonPipelineResult result = m_camera.getLatestResult();\n        \n        if (result.hasTargets()) {\n            PhotonTrackedTarget bestTarget = result.getBestTarget();\n            int fiducialId = bestTarget.getFiducialId();\n            \n            if (fiducialId >= 0) {\n                return Optional.of(fiducialId);\n            }\n        }\n        \n        return Optional.empty();\n    }\n    \n    /**\n     * Check if any AprilTags are currently detected.\n     * \n     * @return true if targets are detected\n     */\n    public boolean hasTargets() {\n        PhotonPipelineResult result = m_camera.getLatestResult();\n        return result.hasTargets();\n    }\n}"
                },
                {
                    "label": "Limelight",
                    "code": "package frc.robot.subsystems;\n\nimport edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.wpilibj2.command.SubsystemBase;\nimport frc.robot.LimelightHelpers;\nimport frc.robot.LimelightHelpers.PoseEstimate;\nimport frc.robot.LimelightHelpers.LimelightResults;\nimport java.util.Optional;\n\n/**\n * Vision subsystem for AprilTag detection using Limelight.\n * \n * IMPORTANT: Before using this subsystem:\n * 1. Set pipeline to \"Fiducial Markers\" in Limelight UI (pipelines 0-9)\n * 2. Configure camera name (use \"limelight\" for default camera)\n * 3. Ensure LimelightHelpers.java is in your project\n *    Download from: https://docs.limelightvision.io/docs/docs-limelight/apis-and-libraries\n */\npublic class VisionSubsystem extends SubsystemBase {\n    private final String m_limelightName;\n    private final int m_pipelineIndex;\n    \n    /**\n     * Create a new VisionSubsystem with default camera name.\n     */\n    public VisionSubsystem() {\n        this(\"limelight\", 0);\n    }\n    \n    /**\n     * Create a new VisionSubsystem.\n     * @param limelightName Name of the Limelight camera (from Limelight UI)\n     * @param pipelineIndex Pipeline index (0-9) configured for Fiducial Markers\n     */\n    public VisionSubsystem(String limelightName, int pipelineIndex) {\n        m_limelightName = limelightName;\n        m_pipelineIndex = pipelineIndex;\n        \n        // Configure Limelight pipeline\n        // Set pipeline type to \"Fiducial Markers\" in Limelight UI\n        LimelightHelpers.setPipelineIndex(m_limelightName, m_pipelineIndex);\n        \n        // Configure camera pose relative to robot center\n        // Parameters: Forward, Side, Up, Roll, Pitch, Yaw\n        // Measure your camera's position relative to robot center\n        LimelightHelpers.setCameraPose_RobotSpace(\n            m_limelightName,\n            0.3,   // Forward (meters) - positive = forward from robot center\n            0.0,   // Side (meters) - positive = left of robot center\n            0.2,   // Up (meters) - positive = above robot center\n            0.0,   // Roll (degrees) - rotation around forward axis\n            0.0,   // Pitch (degrees) - rotation around side axis\n            0.0    // Yaw (degrees) - rotation around up axis\n        );\n    }\n    \n    /**\n     * Get pose estimate with full metadata (recommended for sensor fusion).\n     * This includes timestamp, tag count, and quality metrics.\n     * \n     * @return Optional containing PoseEstimate if valid\n     */\n    public Optional<PoseEstimate> getAprilTagPoseEstimate() {\n        // Get MegaTag pose estimate in WPILib Blue coordinate system\n        // For 2024+, always use wpiBlue regardless of alliance\n        PoseEstimate poseEstimate = \n            LimelightHelpers.getBotPoseEstimate_wpiBlue(m_limelightName);\n        \n        // Validate the pose estimate\n        // Checks that tags were detected and pose is valid\n        if (LimelightHelpers.validPoseEstimate(poseEstimate)) {\n            return Optional.of(poseEstimate);\n        }\n        \n        return Optional.empty();\n    }\n    \n    /**\n     * Get just the pose (2D position and heading).\n     * Use this if you only need the pose without metadata.\n     * \n     * @return Optional containing Pose2d if available\n     */\n    public Optional<Pose2d> getAprilTagPose() {\n        Optional<PoseEstimate> estimate = getAprilTagPoseEstimate();\n        \n        if (estimate.isPresent()) {\n            return Optional.of(estimate.get().pose);\n        }\n        \n        return Optional.empty();\n    }\n    \n    /**\n     * Get the ID of the detected AprilTag.\n     * \n     * @return Optional containing tag ID if detected\n     */\n    public Optional<Integer> getDetectedTagId() {\n        LimelightResults results = \n            LimelightHelpers.getLatestResults(m_limelightName);\n        \n        // Check if any fiducial targets are detected\n        if (results.valid \n                && results.targets_Fiducials.length > 0) {\n            LimelightHelpers.LimelightTarget_Fiducial target = \n                results.targets_Fiducials[0];\n            \n            return Optional.of((int) target.fiducialID);\n        }\n        \n        return Optional.empty();\n    }\n    \n    /**\n     * Check if any AprilTags are currently detected.\n     * \n     * @return true if targets are detected\n     */\n    public boolean hasTargets() {\n        LimelightResults results = \n            LimelightHelpers.getLatestResults(m_limelightName);\n        return results.valid \n            && results.targets_Fiducials.length > 0;\n    }\n    \n    /**\n     * Check if a valid pose estimate is available.\n     * \n     * @return true if valid pose estimate available\n     */\n    public boolean hasValidPoseEstimate() {\n        PoseEstimate estimate = \n            LimelightHelpers.getBotPoseEstimate_wpiBlue(m_limelightName);\n        return LimelightHelpers.validPoseEstimate(estimate);\n    }\n}"
                },
                {
                    "label": "LimelightHelpers.java",
                    "code": "//LimelightHelpers v1.12 (REQUIRES LLOS 2025.0 OR LATER)\n\npackage frc.robot;\n\nimport edu.wpi.first.networktables.DoubleArrayEntry;\nimport edu.wpi.first.networktables.NetworkTable;\nimport edu.wpi.first.networktables.NetworkTableEntry;\nimport edu.wpi.first.networktables.NetworkTableInstance;\nimport edu.wpi.first.networktables.TimestampedDoubleArray;\nimport frc.robot.LimelightHelpers.LimelightResults;\nimport frc.robot.LimelightHelpers.PoseEstimate;\nimport edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.math.geometry.Pose3d;\nimport edu.wpi.first.math.geometry.Rotation2d;\nimport edu.wpi.first.math.geometry.Translation3d;\nimport edu.wpi.first.math.util.Units;\nimport edu.wpi.first.math.geometry.Rotation3d;\nimport edu.wpi.first.math.geometry.Translation2d;\n\nimport java.io.IOException;\nimport java.net.HttpURLConnection;\nimport java.net.MalformedURLException;\nimport java.net.URL;\nimport java.util.Arrays;\nimport java.util.Map;\nimport java.util.concurrent.CompletableFuture;\n\nimport com.fasterxml.jackson.annotation.JsonFormat;\nimport com.fasterxml.jackson.annotation.JsonFormat.Shape;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.databind.DeserializationFeature;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport java.util.concurrent.ConcurrentHashMap;\n\n/**\n * LimelightHelpers provides static methods and classes for interfacing with Limelight vision cameras in FRC.\n * This library supports all Limelight features including AprilTag tracking, Neural Networks, and standard color/retroreflective tracking.\n */\npublic class LimelightHelpers {\n\n    private static final Map<String, DoubleArrayEntry> doubleArrayEntries = new ConcurrentHashMap<>();\n\n    /**\n     * Represents a Color/Retroreflective Target Result extracted from JSON Output\n     */\n    public static class LimelightTarget_Retro {\n\n        @JsonProperty(\"t6c_ts\")\n        private double[] cameraPose_TargetSpace;\n\n        @JsonProperty(\"t6r_fs\")\n        private double[] robotPose_FieldSpace;\n\n        @JsonProperty(\"t6r_ts\")\n        private  double[] robotPose_TargetSpace;\n\n        @JsonProperty(\"t6t_cs\")\n        private double[] targetPose_CameraSpace;\n\n        @JsonProperty(\"t6t_rs\")\n        private double[] targetPose_RobotSpace;\n\n        public Pose3d getCameraPose_TargetSpace()\n        {\n            return toPose3D(cameraPose_TargetSpace);\n        }\n        public Pose3d getRobotPose_FieldSpace()\n        {\n            return toPose3D(robotPose_FieldSpace);\n        }\n        public Pose3d getRobotPose_TargetSpace()\n        {\n            return toPose3D(robotPose_TargetSpace);\n        }\n        public Pose3d getTargetPose_CameraSpace()\n        {\n            return toPose3D(targetPose_CameraSpace);\n        }\n        public Pose3d getTargetPose_RobotSpace()\n        {\n            return toPose3D(targetPose_RobotSpace);\n        }\n\n        public Pose2d getCameraPose_TargetSpace2D()\n        {\n            return toPose2D(cameraPose_TargetSpace);\n        }\n        public Pose2d getRobotPose_FieldSpace2D()\n        {\n            return toPose2D(robotPose_FieldSpace);\n        }\n        public Pose2d getRobotPose_TargetSpace2D()\n        {\n            return toPose2D(robotPose_TargetSpace);\n        }\n        public Pose2d getTargetPose_CameraSpace2D()\n        {\n            return toPose2D(targetPose_CameraSpace);\n        }\n        public Pose2d getTargetPose_RobotSpace2D()\n        {\n            return toPose2D(targetPose_RobotSpace);\n        }\n\n        @JsonProperty(\"ta\")\n        public double ta;\n\n        @JsonProperty(\"tx\")\n        public double tx;\n        \n        @JsonProperty(\"ty\")\n        public double ty;\n\n        @JsonProperty(\"txp\")\n        public double tx_pixels;\n\n        @JsonProperty(\"typ\")\n        public double ty_pixels;\n\n        @JsonProperty(\"tx_nocross\")\n        public double tx_nocrosshair;\n\n        @JsonProperty(\"ty_nocross\")\n        public double ty_nocrosshair;\n\n        @JsonProperty(\"ts\")\n        public double ts;\n\n        public LimelightTarget_Retro() {\n            cameraPose_TargetSpace = new double[6];\n            robotPose_FieldSpace = new double[6];\n            robotPose_TargetSpace = new double[6];\n            targetPose_CameraSpace = new double[6];\n            targetPose_RobotSpace = new double[6];\n        }\n\n    }\n\n    /**\n     * Represents an AprilTag/Fiducial Target Result extracted from JSON Output\n     */\n    public static class LimelightTarget_Fiducial {\n\n        @JsonProperty(\"fID\")\n        public double fiducialID;\n\n        @JsonProperty(\"fam\")\n        public String fiducialFamily;\n\n        @JsonProperty(\"t6c_ts\")\n        private double[] cameraPose_TargetSpace;\n\n        @JsonProperty(\"t6r_fs\")\n        private double[] robotPose_FieldSpace;\n\n        @JsonProperty(\"t6r_ts\")\n        private double[] robotPose_TargetSpace;\n\n        @JsonProperty(\"t6t_cs\")\n        private double[] targetPose_CameraSpace;\n\n        @JsonProperty(\"t6t_rs\")\n        private double[] targetPose_RobotSpace;\n\n        public Pose3d getCameraPose_TargetSpace()\n        {\n            return toPose3D(cameraPose_TargetSpace);\n        }\n        public Pose3d getRobotPose_FieldSpace()\n        {\n            return toPose3D(robotPose_FieldSpace);\n        }\n        public Pose3d getRobotPose_TargetSpace()\n        {\n            return toPose3D(robotPose_TargetSpace);\n        }\n        public Pose3d getTargetPose_CameraSpace()\n        {\n            return toPose3D(targetPose_CameraSpace);\n        }\n        public Pose3d getTargetPose_RobotSpace()\n        {\n            return toPose3D(targetPose_RobotSpace);\n        }\n\n        public Pose2d getCameraPose_TargetSpace2D()\n        {\n            return toPose2D(cameraPose_TargetSpace);\n        }\n        public Pose2d getRobotPose_FieldSpace2D()\n        {\n            return toPose2D(robotPose_FieldSpace);\n        }\n        public Pose2d getRobotPose_TargetSpace2D()\n        {\n            return toPose2D(robotPose_TargetSpace);\n        }\n        public Pose2d getTargetPose_CameraSpace2D()\n        {\n            return toPose2D(targetPose_CameraSpace);\n        }\n        public Pose2d getTargetPose_RobotSpace2D()\n        {\n            return toPose2D(targetPose_RobotSpace);\n        }\n        \n        @JsonProperty(\"ta\")\n        public double ta;\n\n        @JsonProperty(\"tx\")\n        public double tx;\n\n        @JsonProperty(\"ty\")\n        public double ty;\n\n        @JsonProperty(\"txp\")\n        public double tx_pixels;\n\n        @JsonProperty(\"typ\")\n        public double ty_pixels;\n\n        @JsonProperty(\"tx_nocross\")\n        public double tx_nocrosshair;\n\n        @JsonProperty(\"ty_nocross\")\n        public double ty_nocrosshair;\n\n        @JsonProperty(\"ts\")\n        public double ts;\n        \n        public LimelightTarget_Fiducial() {\n            cameraPose_TargetSpace = new double[6];\n            robotPose_FieldSpace = new double[6];\n            robotPose_TargetSpace = new double[6];\n            targetPose_CameraSpace = new double[6];\n            targetPose_RobotSpace = new double[6];\n        }\n    }\n\n    /**\n     * Represents a Barcode Target Result extracted from JSON Output\n     */\n    public static class LimelightTarget_Barcode {\n\n        /**\n         * Barcode family type (e.g. \"QR\", \"DataMatrix\", etc.)\n         */\n        @JsonProperty(\"fam\")\n        public String family;\n\n        /**\n         * Gets the decoded data content of the barcode\n         */\n        @JsonProperty(\"data\") \n        public String data;\n\n        @JsonProperty(\"txp\")\n        public double tx_pixels;\n\n        @JsonProperty(\"typ\")\n        public double ty_pixels;\n\n        @JsonProperty(\"tx\")\n        public double tx;\n\n        @JsonProperty(\"ty\")\n        public double ty;\n\n        @JsonProperty(\"tx_nocross\")\n        public double tx_nocrosshair;\n\n        @JsonProperty(\"ty_nocross\")\n        public double ty_nocrosshair;\n\n        @JsonProperty(\"ta\")\n        public double ta;\n\n        @JsonProperty(\"pts\")\n        public double[][] corners;\n\n        public LimelightTarget_Barcode() {\n        }\n\n        public String getFamily() {\n            return family;\n        }\n    }\n\n    /**\n     * Represents a Neural Classifier Pipeline Result extracted from JSON Output\n     */\n    public static class LimelightTarget_Classifier {\n\n        @JsonProperty(\"class\")\n        public String className;\n\n        @JsonProperty(\"classID\")\n        public double classID;\n\n        @JsonProperty(\"conf\")\n        public double confidence;\n\n        @JsonProperty(\"zone\")\n        public double zone;\n\n        @JsonProperty(\"tx\")\n        public double tx;\n\n        @JsonProperty(\"txp\")\n        public double tx_pixels;\n\n        @JsonProperty(\"ty\")\n        public double ty;\n\n        @JsonProperty(\"typ\")\n        public double ty_pixels;\n\n        public  LimelightTarget_Classifier() {\n        }\n    }\n\n    /**\n     * Represents a Neural Detector Pipeline Result extracted from JSON Output\n     */\n    public static class LimelightTarget_Detector {\n\n        @JsonProperty(\"class\")\n        public String className;\n\n        @JsonProperty(\"classID\")\n        public double classID;\n\n        @JsonProperty(\"conf\")\n        public double confidence;\n\n        @JsonProperty(\"ta\")\n        public double ta;\n\n        @JsonProperty(\"tx\")\n        public double tx;\n\n        @JsonProperty(\"ty\")\n        public double ty;\n\n        @JsonProperty(\"txp\")\n        public double tx_pixels;\n\n        @JsonProperty(\"typ\")\n        public double ty_pixels;\n\n        @JsonProperty(\"tx_nocross\")\n        public double tx_nocrosshair;\n\n        @JsonProperty(\"ty_nocross\")\n        public double ty_nocrosshair;\n\n        public LimelightTarget_Detector() {\n        }\n    }\n\n    /**\n     * Limelight Results object, parsed from a Limelight's JSON results output.\n     */\n    public static class LimelightResults {\n        \n        public String error;\n        \n        @JsonProperty(\"pID\")\n        public double pipelineID;\n\n        @JsonProperty(\"tl\")\n        public double latency_pipeline;\n\n        @JsonProperty(\"cl\")\n        public double latency_capture;\n\n        public double latency_jsonParse;\n\n        @JsonProperty(\"ts\")\n        public double timestamp_LIMELIGHT_publish;\n\n        @JsonProperty(\"ts_rio\")\n        public double timestamp_RIOFPGA_capture;\n\n        @JsonProperty(\"v\")\n        @JsonFormat(shape = Shape.NUMBER)\n        public boolean valid;\n\n        @JsonProperty(\"botpose\")\n        public double[] botpose;\n\n        @JsonProperty(\"botpose_wpired\")\n        public double[] botpose_wpired;\n\n        @JsonProperty(\"botpose_wpiblue\")\n        public double[] botpose_wpiblue;\n\n        @JsonProperty(\"botpose_tagcount\")\n        public double botpose_tagcount;\n       \n        @JsonProperty(\"botpose_span\")\n        public double botpose_span;\n       \n        @JsonProperty(\"botpose_avgdist\")\n        public double botpose_avgdist;\n       \n        @JsonProperty(\"botpose_avgarea\")\n        public double botpose_avgarea;\n\n        @JsonProperty(\"t6c_rs\")\n        public double[] camerapose_robotspace;\n\n        public Pose3d getBotPose3d() {\n            return toPose3D(botpose);\n        }\n    \n        public Pose3d getBotPose3d_wpiRed() {\n            return toPose3D(botpose_wpired);\n        }\n    \n        public Pose3d getBotPose3d_wpiBlue() {\n            return toPose3D(botpose_wpiblue);\n        }\n\n        public Pose2d getBotPose2d() {\n            return toPose2D(botpose);\n        }\n    \n        public Pose2d getBotPose2d_wpiRed() {\n            return toPose2D(botpose_wpired);\n        }\n    \n        public Pose2d getBotPose2d_wpiBlue() {\n            return toPose2D(botpose_wpiblue);\n        }\n\n        @JsonProperty(\"Retro\")\n        public LimelightTarget_Retro[] targets_Retro;\n\n        @JsonProperty(\"Fiducial\")\n        public LimelightTarget_Fiducial[] targets_Fiducials;\n\n        @JsonProperty(\"Classifier\")\n        public LimelightTarget_Classifier[] targets_Classifier;\n\n        @JsonProperty(\"Detector\")\n        public LimelightTarget_Detector[] targets_Detector;\n\n        @JsonProperty(\"Barcode\")\n        public LimelightTarget_Barcode[] targets_Barcode;\n\n        public LimelightResults() {\n            botpose = new double[6];\n            botpose_wpired = new double[6];\n            botpose_wpiblue = new double[6];\n            camerapose_robotspace = new double[6];\n            targets_Retro = new LimelightTarget_Retro[0];\n            targets_Fiducials = new LimelightTarget_Fiducial[0];\n            targets_Classifier = new LimelightTarget_Classifier[0];\n            targets_Detector = new LimelightTarget_Detector[0];\n            targets_Barcode = new LimelightTarget_Barcode[0];\n\n        }\n\n\n\n    }\n\n    /**\n     * Represents a Limelight Raw Fiducial result from Limelight's NetworkTables output.\n     */\n    public static class RawFiducial {\n        public int id = 0;\n        public double txnc = 0;\n        public double tync = 0;\n        public double ta = 0;\n        public double distToCamera = 0;\n        public double distToRobot = 0;\n        public double ambiguity = 0;\n\n\n\n        public RawFiducial(int id, double txnc, double tync, double ta, double distToCamera, double distToRobot, double ambiguity) {\n            this.id = id;\n            this.txnc = txnc;\n            this.tync = tync;\n            this.ta = ta;\n            this.distToCamera = distToCamera;\n            this.distToRobot = distToRobot;\n            this.ambiguity = ambiguity;\n        }\n\n        @Override\n        public boolean equals(Object obj) {\n            if (this == obj) return true;\n            if (obj == null || getClass() != obj.getClass()) return false;\n            RawFiducial other = (RawFiducial) obj;\n            return id == other.id &&\n                Double.compare(txnc, other.txnc) == 0 &&\n                Double.compare(tync, other.tync) == 0 &&\n                Double.compare(ta, other.ta) == 0 &&\n                Double.compare(distToCamera, other.distToCamera) == 0 &&\n                Double.compare(distToRobot, other.distToRobot) == 0 &&\n                Double.compare(ambiguity, other.ambiguity) == 0;\n        }\n\n    }\n\n    /**\n     * Represents a Limelight Raw Neural Detector result from Limelight's NetworkTables output.\n     */\n    public static class RawDetection {\n        public int classId = 0;\n        public double txnc = 0;\n        public double tync = 0;\n        public double ta = 0;\n        public double corner0_X = 0;\n        public double corner0_Y = 0;\n        public double corner1_X = 0;\n        public double corner1_Y = 0;\n        public double corner2_X = 0;\n        public double corner2_Y = 0;\n        public double corner3_X = 0;\n        public double corner3_Y = 0;\n\n\n\n        public RawDetection(int classId, double txnc, double tync, double ta, \n            double corner0_X, double corner0_Y, \n            double corner1_X, double corner1_Y, \n            double corner2_X, double corner2_Y, \n            double corner3_X, double corner3_Y ) {\n            this.classId = classId;\n            this.txnc = txnc;\n            this.tync = tync;\n            this.ta = ta;\n            this.corner0_X = corner0_X;\n            this.corner0_Y = corner0_Y;\n            this.corner1_X = corner1_X;\n            this.corner1_Y = corner1_Y;\n            this.corner2_X = corner2_X;\n            this.corner2_Y = corner2_Y;\n            this.corner3_X = corner3_X;\n            this.corner3_Y = corner3_Y;\n        }\n    }\n    \n    /**\n     * Represents a 3D Pose Estimate.\n     */\n    public static class PoseEstimate {\n        public Pose2d pose;\n        public double timestampSeconds;\n        public double latency;\n        public int tagCount;\n        public double tagSpan;\n        public double avgTagDist;\n        public double avgTagArea;\n\n        public RawFiducial[] rawFiducials; \n        public boolean isMegaTag2;\n\n        /**\n         * Instantiates a PoseEstimate object with default values\n         */\n        public PoseEstimate() {\n            this.pose = new Pose2d();\n            this.timestampSeconds = 0;\n            this.latency = 0;\n            this.tagCount = 0;\n            this.tagSpan = 0;\n            this.avgTagDist = 0;\n            this.avgTagArea = 0;\n            this.rawFiducials = new RawFiducial[]{};\n            this.isMegaTag2 = false;\n        }\n\n        public PoseEstimate(Pose2d pose, double timestampSeconds, double latency, \n            int tagCount, double tagSpan, double avgTagDist, \n            double avgTagArea, RawFiducial[] rawFiducials, boolean isMegaTag2) {\n\n            this.pose = pose;\n            this.timestampSeconds = timestampSeconds;\n            this.latency = latency;\n            this.tagCount = tagCount;\n            this.tagSpan = tagSpan;\n            this.avgTagDist = avgTagDist;\n            this.avgTagArea = avgTagArea;\n            this.rawFiducials = rawFiducials;\n            this.isMegaTag2 = isMegaTag2;\n        }\n\n        @Override\n        public boolean equals(Object obj) {\n            if (this == obj) return true;\n            if (obj == null || getClass() != obj.getClass()) return false;\n            PoseEstimate that = (PoseEstimate) obj;\n            // We don't compare the timestampSeconds as it isn't relevant for equality and makes\n            // unit testing harder\n            return Double.compare(that.latency, latency) == 0\n                && tagCount == that.tagCount\n                && Double.compare(that.tagSpan, tagSpan) == 0\n                && Double.compare(that.avgTagDist, avgTagDist) == 0\n                && Double.compare(that.avgTagArea, avgTagArea) == 0\n                && pose.equals(that.pose)\n                && Arrays.equals(rawFiducials, that.rawFiducials);\n        }\n\n    }\n\n    /**\n     * Encapsulates the state of an internal Limelight IMU.\n     */\n    public static class IMUData {\n        public double robotYaw = 0.0;\n        public double Roll = 0.0;\n        public double Pitch = 0.0;\n        public double Yaw = 0.0;\n        public double gyroX = 0.0;\n        public double gyroY = 0.0;\n        public double gyroZ = 0.0;\n        public double accelX = 0.0;\n        public double accelY = 0.0;\n        public double accelZ = 0.0;\n\n        public IMUData() {}\n\n        public IMUData(double[] imuData) {\n            if (imuData != null && imuData.length >= 10) {\n                this.robotYaw = imuData[0];\n                this.Roll = imuData[1];\n                this.Pitch = imuData[2];\n                this.Yaw = imuData[3];\n                this.gyroX = imuData[4];\n                this.gyroY = imuData[5];\n                this.gyroZ = imuData[6];\n                this.accelX = imuData[7];\n                this.accelY = imuData[8];\n                this.accelZ = imuData[9];\n            }\n        }\n    }\n\n\n\n    private static ObjectMapper mapper;\n\n    /**\n     * Print JSON Parse time to the console in milliseconds\n     */\n    static boolean profileJSON = false;\n\n    static final String sanitizeName(String name) {\n        if (\"\".equals(name) || name == null) {\n            return \"limelight\";\n        }\n        return name;\n    }\n\n    /**\n     * Takes a 6-length array of pose data and converts it to a Pose3d object.\n     * Array format: [x, y, z, roll, pitch, yaw] where angles are in degrees.\n     * @param inData Array containing pose data [x, y, z, roll, pitch, yaw]\n     * @return Pose3d object representing the pose, or empty Pose3d if invalid data\n     */\n    public static Pose3d toPose3D(double[] inData){\n        if(inData.length < 6)\n        {\n            //System.err.println(\"Bad LL 3D Pose Data!\");\n            return new Pose3d();\n        }\n        return new Pose3d(\n            new Translation3d(inData[0], inData[1], inData[2]),\n            new Rotation3d(Units.degreesToRadians(inData[3]), Units.degreesToRadians(inData[4]),\n                    Units.degreesToRadians(inData[5])));\n    }\n\n    /**\n     * Takes a 6-length array of pose data and converts it to a Pose2d object.\n     * Uses only x, y, and yaw components, ignoring z, roll, and pitch.\n     * Array format: [x, y, z, roll, pitch, yaw] where angles are in degrees.\n     * @param inData Array containing pose data [x, y, z, roll, pitch, yaw]\n     * @return Pose2d object representing the pose, or empty Pose2d if invalid data\n     */\n    public static Pose2d toPose2D(double[] inData){\n        if(inData.length < 6)\n        {\n            //System.err.println(\"Bad LL 2D Pose Data!\");\n            return new Pose2d();\n        }\n        Translation2d tran2d = new Translation2d(inData[0], inData[1]);\n        Rotation2d r2d = new Rotation2d(Units.degreesToRadians(inData[5]));\n        return new Pose2d(tran2d, r2d);\n    }\n\n    /**\n     * Converts a Pose3d object to an array of doubles in the format [x, y, z, roll, pitch, yaw].\n     * Translation components are in meters, rotation components are in degrees.\n     * \n     * @param pose The Pose3d object to convert\n     * @return A 6-element array containing [x, y, z, roll, pitch, yaw]\n     */\n    public static double[] pose3dToArray(Pose3d pose) {\n        double[] result = new double[6];\n        result[0] = pose.getTranslation().getX();\n        result[1] = pose.getTranslation().getY();\n        result[2] = pose.getTranslation().getZ();\n        result[3] = Units.radiansToDegrees(pose.getRotation().getX());\n        result[4] = Units.radiansToDegrees(pose.getRotation().getY());\n        result[5] = Units.radiansToDegrees(pose.getRotation().getZ());\n        return result;\n    }\n\n    /**\n     * Converts a Pose2d object to an array of doubles in the format [x, y, z, roll, pitch, yaw].\n     * Translation components are in meters, rotation components are in degrees.\n     * Note: z, roll, and pitch will be 0 since Pose2d only contains x, y, and yaw.\n     * \n     * @param pose The Pose2d object to convert\n     * @return A 6-element array containing [x, y, 0, 0, 0, yaw]\n     */\n    public static double[] pose2dToArray(Pose2d pose) {\n        double[] result = new double[6];\n        result[0] = pose.getTranslation().getX();\n        result[1] = pose.getTranslation().getY();\n        result[2] = 0;\n        result[3] = Units.radiansToDegrees(0);\n        result[4] = Units.radiansToDegrees(0);\n        result[5] = Units.radiansToDegrees(pose.getRotation().getRadians());\n        return result;\n    }\n\n    private static double extractArrayEntry(double[] inData, int position){\n        if(inData.length < position+1)\n        {\n            return 0;\n        }\n        return inData[position];\n    }\n\n    private static PoseEstimate getBotPoseEstimate(String limelightName, String entryName, boolean isMegaTag2) {\n        DoubleArrayEntry poseEntry = LimelightHelpers.getLimelightDoubleArrayEntry(limelightName, entryName);\n        \n        TimestampedDoubleArray tsValue = poseEntry.getAtomic();\n        double[] poseArray = tsValue.value;\n        long timestamp = tsValue.timestamp;\n        \n        if (poseArray.length == 0) {\n            // Handle the case where no data is available\n            return null; // or some default PoseEstimate\n        }\n    \n        var pose = toPose2D(poseArray);\n        double latency = extractArrayEntry(poseArray, 6);\n        int tagCount = (int)extractArrayEntry(poseArray, 7);\n        double tagSpan = extractArrayEntry(poseArray, 8);\n        double tagDist = extractArrayEntry(poseArray, 9);\n        double tagArea = extractArrayEntry(poseArray, 10);\n        \n        // Convert server timestamp from microseconds to seconds and adjust for latency\n        double adjustedTimestamp = (timestamp / 1000000.0) - (latency / 1000.0);\n    \n        RawFiducial[] rawFiducials = new RawFiducial[tagCount];\n        int valsPerFiducial = 7;\n        int expectedTotalVals = 11 + valsPerFiducial * tagCount;\n    \n        if (poseArray.length != expectedTotalVals) {\n            // Don't populate fiducials\n        } else {\n            for(int i = 0; i < tagCount; i++) {\n                int baseIndex = 11 + (i * valsPerFiducial);\n                int id = (int)poseArray[baseIndex];\n                double txnc = poseArray[baseIndex + 1];\n                double tync = poseArray[baseIndex + 2];\n                double ta = poseArray[baseIndex + 3];\n                double distToCamera = poseArray[baseIndex + 4];\n                double distToRobot = poseArray[baseIndex + 5];\n                double ambiguity = poseArray[baseIndex + 6];\n                rawFiducials[i] = new RawFiducial(id, txnc, tync, ta, distToCamera, distToRobot, ambiguity);\n            }\n        }\n    \n        return new PoseEstimate(pose, adjustedTimestamp, latency, tagCount, tagSpan, tagDist, tagArea, rawFiducials, isMegaTag2);\n    }\n\n    /**\n     * Gets the latest raw fiducial/AprilTag detection results from NetworkTables.\n     * \n     * @param limelightName Name/identifier of the Limelight\n     * @return Array of RawFiducial objects containing detection details\n     */\n    public static RawFiducial[] getRawFiducials(String limelightName) {\n        var entry = LimelightHelpers.getLimelightNTTableEntry(limelightName, \"rawfiducials\");\n        var rawFiducialArray = entry.getDoubleArray(new double[0]);\n        int valsPerEntry = 7;\n        if (rawFiducialArray.length % valsPerEntry != 0) {\n            return new RawFiducial[0];\n        }\n    \n        int numFiducials = rawFiducialArray.length / valsPerEntry;\n        RawFiducial[] rawFiducials = new RawFiducial[numFiducials];\n    \n        for (int i = 0; i < numFiducials; i++) {\n            int baseIndex = i * valsPerEntry;\n            int id = (int) extractArrayEntry(rawFiducialArray, baseIndex);\n            double txnc = extractArrayEntry(rawFiducialArray, baseIndex + 1);\n            double tync = extractArrayEntry(rawFiducialArray, baseIndex + 2);\n            double ta = extractArrayEntry(rawFiducialArray, baseIndex + 3);\n            double distToCamera = extractArrayEntry(rawFiducialArray, baseIndex + 4);\n            double distToRobot = extractArrayEntry(rawFiducialArray, baseIndex + 5);\n            double ambiguity = extractArrayEntry(rawFiducialArray, baseIndex + 6);\n            \n            rawFiducials[i] = new RawFiducial(id, txnc, tync, ta, distToCamera, distToRobot, ambiguity);\n        }\n    \n        return rawFiducials;\n    }\n\n    /**\n     * Gets the latest raw neural detector results from NetworkTables\n     *\n     * @param limelightName Name/identifier of the Limelight\n     * @return Array of RawDetection objects containing detection details\n     */\n    public static RawDetection[] getRawDetections(String limelightName) {\n        var entry = LimelightHelpers.getLimelightNTTableEntry(limelightName, \"rawdetections\");\n        var rawDetectionArray = entry.getDoubleArray(new double[0]);\n        int valsPerEntry = 12;\n        if (rawDetectionArray.length % valsPerEntry != 0) {\n            return new RawDetection[0];\n        }\n    \n        int numDetections = rawDetectionArray.length / valsPerEntry;\n        RawDetection[] rawDetections = new RawDetection[numDetections];\n    \n        for (int i = 0; i < numDetections; i++) {\n            int baseIndex = i * valsPerEntry; // Starting index for this detection's data\n            int classId = (int) extractArrayEntry(rawDetectionArray, baseIndex);\n            double txnc = extractArrayEntry(rawDetectionArray, baseIndex + 1);\n            double tync = extractArrayEntry(rawDetectionArray, baseIndex + 2);\n            double ta = extractArrayEntry(rawDetectionArray, baseIndex + 3);\n            double corner0_X = extractArrayEntry(rawDetectionArray, baseIndex + 4);\n            double corner0_Y = extractArrayEntry(rawDetectionArray, baseIndex + 5);\n            double corner1_X = extractArrayEntry(rawDetectionArray, baseIndex + 6);\n            double corner1_Y = extractArrayEntry(rawDetectionArray, baseIndex + 7);\n            double corner2_X = extractArrayEntry(rawDetectionArray, baseIndex + 8);\n            double corner2_Y = extractArrayEntry(rawDetectionArray, baseIndex + 9);\n            double corner3_X = extractArrayEntry(rawDetectionArray, baseIndex + 10);\n            double corner3_Y = extractArrayEntry(rawDetectionArray, baseIndex + 11);\n            \n            rawDetections[i] = new RawDetection(classId, txnc, tync, ta, corner0_X, corner0_Y, corner1_X, corner1_Y, corner2_X, corner2_Y, corner3_X, corner3_Y);\n        }\n    \n        return rawDetections;\n    }\n\n    /**\n     * Prints detailed information about a PoseEstimate to standard output.\n     * Includes timestamp, latency, tag count, tag span, average tag distance,\n     * average tag area, and detailed information about each detected fiducial.\n     *\n     * @param pose The PoseEstimate object to print. If null, prints \"No PoseEstimate available.\"\n     */\n    public static void printPoseEstimate(PoseEstimate pose) {\n        if (pose == null) {\n            System.out.println(\"No PoseEstimate available.\");\n            return;\n        }\n    \n        System.out.printf(\"Pose Estimate Information:%n\");\n        System.out.printf(\"Timestamp (Seconds): %.3f%n\", pose.timestampSeconds);\n        System.out.printf(\"Latency: %.3f ms%n\", pose.latency);\n        System.out.printf(\"Tag Count: %d%n\", pose.tagCount);\n        System.out.printf(\"Tag Span: %.2f meters%n\", pose.tagSpan);\n        System.out.printf(\"Average Tag Distance: %.2f meters%n\", pose.avgTagDist);\n        System.out.printf(\"Average Tag Area: %.2f%% of image%n\", pose.avgTagArea);\n        System.out.printf(\"Is MegaTag2: %b%n\", pose.isMegaTag2);\n        System.out.println();\n    \n        if (pose.rawFiducials == null || pose.rawFiducials.length == 0) {\n            System.out.println(\"No RawFiducials data available.\");\n            return;\n        }\n    \n        System.out.println(\"Raw Fiducials Details:\");\n        for (int i = 0; i < pose.rawFiducials.length; i++) {\n            RawFiducial fiducial = pose.rawFiducials[i];\n            System.out.printf(\" Fiducial #%d:%n\", i + 1);\n            System.out.printf(\"  ID: %d%n\", fiducial.id);\n            System.out.printf(\"  TXNC: %.2f%n\", fiducial.txnc);\n            System.out.printf(\"  TYNC: %.2f%n\", fiducial.tync);\n            System.out.printf(\"  TA: %.2f%n\", fiducial.ta);\n            System.out.printf(\"  Distance to Camera: %.2f meters%n\", fiducial.distToCamera);\n            System.out.printf(\"  Distance to Robot: %.2f meters%n\", fiducial.distToRobot);\n            System.out.printf(\"  Ambiguity: %.2f%n\", fiducial.ambiguity);\n            System.out.println();\n        }\n    }\n\n    public static Boolean validPoseEstimate(PoseEstimate pose) {\n        return pose != null && pose.rawFiducials != null && pose.rawFiducials.length != 0;\n    }\n\n    public static NetworkTable getLimelightNTTable(String tableName) {\n        return NetworkTableInstance.getDefault().getTable(sanitizeName(tableName));\n    }\n\n    public static void Flush() {\n        NetworkTableInstance.getDefault().flush();\n    }\n\n    public static NetworkTableEntry getLimelightNTTableEntry(String tableName, String entryName) {\n        return getLimelightNTTable(tableName).getEntry(entryName);\n    }\n\n    public static DoubleArrayEntry getLimelightDoubleArrayEntry(String tableName, String entryName) {\n        String key = tableName + \"/\" + entryName;\n        return doubleArrayEntries.computeIfAbsent(key, k -> {\n            NetworkTable table = getLimelightNTTable(tableName);\n            return table.getDoubleArrayTopic(entryName).getEntry(new double[0]);\n        });\n    }\n    \n    public static double getLimelightNTDouble(String tableName, String entryName) {\n        return getLimelightNTTableEntry(tableName, entryName).getDouble(0.0);\n    }\n\n    public static void setLimelightNTDouble(String tableName, String entryName, double val) {\n        getLimelightNTTableEntry(tableName, entryName).setDouble(val);\n    }\n\n    public static void setLimelightNTDoubleArray(String tableName, String entryName, double[] val) {\n        getLimelightNTTableEntry(tableName, entryName).setDoubleArray(val);\n    }\n\n    public static double[] getLimelightNTDoubleArray(String tableName, String entryName) {\n        return getLimelightNTTableEntry(tableName, entryName).getDoubleArray(new double[0]);\n    }\n\n\n\n    public static String getLimelightNTString(String tableName, String entryName) {\n        return getLimelightNTTableEntry(tableName, entryName).getString(\"\");\n    }\n\n    public static String[] getLimelightNTStringArray(String tableName, String entryName) {\n        return getLimelightNTTableEntry(tableName, entryName).getStringArray(new String[0]);\n    }\n\n\n\n    public static URL getLimelightURLString(String tableName, String request) {\n        String urlString = \"http://\" + sanitizeName(tableName) + \".local:5807/\" + request;\n        URL url;\n        try {\n            url = new URL(urlString);\n            return url;\n        } catch (MalformedURLException e) {\n            System.err.println(\"bad LL URL\");\n        }\n        return null;\n    }\n    /////\n    /////\n\n    /**\n     * Does the Limelight have a valid target?\n     * @param limelightName Name of the Limelight camera (\"\" for default)\n     * @return True if a valid target is present, false otherwise\n     */\n    public static boolean getTV(String limelightName) {\n        return 1.0 == getLimelightNTDouble(limelightName, \"tv\");\n    }\n\n    /**\n     * Gets the horizontal offset from the crosshair to the target in degrees.\n     * @param limelightName Name of the Limelight camera (\"\" for default)\n     * @return Horizontal offset angle in degrees\n     */\n    public static double getTX(String limelightName) {\n        return getLimelightNTDouble(limelightName, \"tx\");\n    }\n\n    /**\n     * Gets the vertical offset from the crosshair to the target in degrees.\n     * @param limelightName Name of the Limelight camera (\"\" for default)\n     * @return Vertical offset angle in degrees\n     */\n    public static double getTY(String limelightName) {\n        return getLimelightNTDouble(limelightName, \"ty\");\n    }\n\n    /**\n     * Gets the horizontal offset from the principal pixel/point to the target in degrees.  This is the most accurate 2d metric if you are using a calibrated camera and you don't need adjustable crosshair functionality.\n     * @param limelightName Name of the Limelight camera (\"\" for default)\n     * @return Horizontal offset angle in degrees\n     */\n    public static double getTXNC(String limelightName) {\n        return getLimelightNTDouble(limelightName, \"txnc\");\n    }\n\n    /**\n     * Gets the vertical offset from the principal pixel/point to the target in degrees. This is the most accurate 2d metric if you are using a calibrated camera and you don't need adjustable crosshair functionality.\n     * @param limelightName Name of the Limelight camera (\"\" for default)\n     * @return Vertical offset angle in degrees\n     */\n    public static double getTYNC(String limelightName) {\n        return getLimelightNTDouble(limelightName, \"tync\");\n    }\n\n    /**\n     * Gets the target area as a percentage of the image (0-100%).\n     * @param limelightName Name of the Limelight camera (\"\" for default) \n     * @return Target area percentage (0-100)\n     */\n    public static double getTA(String limelightName) {\n        return getLimelightNTDouble(limelightName, \"ta\");\n    }\n\n    /**\n     * T2D is an array that contains several targeting metrcis\n     * @param limelightName Name of the Limelight camera\n     * @return Array containing  [targetValid, targetCount, targetLatency, captureLatency, tx, ty, txnc, tync, ta, tid, targetClassIndexDetector, \n     * targetClassIndexClassifier, targetLongSidePixels, targetShortSidePixels, targetHorizontalExtentPixels, targetVerticalExtentPixels, targetSkewDegrees]\n     */\n    public static double[] getT2DArray(String limelightName) {\n        return getLimelightNTDoubleArray(limelightName, \"t2d\");\n    }\n    \n    /**\n     * Gets the number of targets currently detected.\n     * @param limelightName Name of the Limelight camera\n     * @return Number of detected targets\n     */\n    public static int getTargetCount(String limelightName) {\n      double[] t2d = getT2DArray(limelightName);\n      if(t2d.length == 17)\n      {\n        return (int)t2d[1];\n      }\n      return 0;\n    }\n\n    /**\n     * Gets the classifier class index from the currently running neural classifier pipeline\n     * @param limelightName Name of the Limelight camera\n     * @return Class index from classifier pipeline\n     */\n    public static int getClassifierClassIndex (String limelightName) {\n    double[] t2d = getT2DArray(limelightName);\n      if(t2d.length == 17)\n      {\n        return (int)t2d[10];\n      }\n      return 0;\n    }\n\n    /**\n     * Gets the detector class index from the primary result of the currently running neural detector pipeline.\n     * @param limelightName Name of the Limelight camera\n     * @return Class index from detector pipeline\n     */\n    public static int getDetectorClassIndex (String limelightName) {\n     double[] t2d = getT2DArray(limelightName);\n      if(t2d.length == 17)\n      {\n        return (int)t2d[11];\n      }\n      return 0;\n    }\n\n    /**\n     * Gets the current neural classifier result class name.\n     * @param limelightName Name of the Limelight camera\n     * @return Class name string from classifier pipeline\n     */\n    public static String getClassifierClass (String limelightName) {\n        return getLimelightNTString(limelightName, \"tcclass\");\n    }\n\n    /**\n     * Gets the primary neural detector result class name.\n     * @param limelightName Name of the Limelight camera\n     * @return Class name string from detector pipeline\n     */\n    public static String getDetectorClass (String limelightName) {\n        return getLimelightNTString(limelightName, \"tdclass\");\n    }\n\n    /**\n     * Gets the pipeline's processing latency contribution.\n     * @param limelightName Name of the Limelight camera\n     * @return Pipeline latency in milliseconds\n     */\n    public static double getLatency_Pipeline(String limelightName) {\n        return getLimelightNTDouble(limelightName, \"tl\");\n    }\n\n    /**\n     * Gets the capture latency.\n     * @param limelightName Name of the Limelight camera\n     * @return Capture latency in milliseconds\n     */\n    public static double getLatency_Capture(String limelightName) {\n        return getLimelightNTDouble(limelightName, \"cl\");\n    }\n\n    /**\n     * Gets the active pipeline index.\n     * @param limelightName Name of the Limelight camera\n     * @return Current pipeline index (0-9)\n     */\n    public static double getCurrentPipelineIndex(String limelightName) {\n        return getLimelightNTDouble(limelightName, \"getpipe\");\n    }\n\n    /**\n     * Gets the current pipeline type.\n     * @param limelightName Name of the Limelight camera\n     * @return Pipeline type string (e.g. \"retro\", \"apriltag\", etc)\n     */\n    public static String getCurrentPipelineType(String limelightName) {\n        return getLimelightNTString(limelightName, \"getpipetype\");\n    }\n\n    /**\n     * Gets the full JSON results dump.\n     * @param limelightName Name of the Limelight camera\n     * @return JSON string containing all current results\n     */\n    public static String getJSONDump(String limelightName) {\n        return getLimelightNTString(limelightName, \"json\");\n    }\n\n    /**\n     * Switch to getBotPose\n     * \n     * @param limelightName\n     * @return\n     */\n    @Deprecated\n    public static double[] getBotpose(String limelightName) {\n        return getLimelightNTDoubleArray(limelightName, \"botpose\");\n    }\n\n    /**\n     * Switch to getBotPose_wpiRed\n     * \n     * @param limelightName\n     * @return\n     */\n    @Deprecated\n    public static double[] getBotpose_wpiRed(String limelightName) {\n        return getLimelightNTDoubleArray(limelightName, \"botpose_wpired\");\n    }\n\n    /**\n     * Switch to getBotPose_wpiBlue\n     * \n     * @param limelightName\n     * @return\n     */\n    @Deprecated\n    public static double[] getBotpose_wpiBlue(String limelightName) {\n        return getLimelightNTDoubleArray(limelightName, \"botpose_wpiblue\");\n    }\n\n    public static double[] getBotPose(String limelightName) {\n        return getLimelightNTDoubleArray(limelightName, \"botpose\");\n    }\n\n    public static double[] getBotPose_wpiRed(String limelightName) {\n        return getLimelightNTDoubleArray(limelightName, \"botpose_wpired\");\n    }\n\n    public static double[] getBotPose_wpiBlue(String limelightName) {\n        return getLimelightNTDoubleArray(limelightName, \"botpose_wpiblue\");\n    }\n\n    public static double[] getBotPose_TargetSpace(String limelightName) {\n        return getLimelightNTDoubleArray(limelightName, \"botpose_targetspace\");\n    }\n\n    public static double[] getCameraPose_TargetSpace(String limelightName) {\n        return getLimelightNTDoubleArray(limelightName, \"camerapose_targetspace\");\n    }\n\n    public static double[] getTargetPose_CameraSpace(String limelightName) {\n        return getLimelightNTDoubleArray(limelightName, \"targetpose_cameraspace\");\n    }\n\n    public static double[] getTargetPose_RobotSpace(String limelightName) {\n        return getLimelightNTDoubleArray(limelightName, \"targetpose_robotspace\");\n    }\n\n    public static double[] getTargetColor(String limelightName) {\n        return getLimelightNTDoubleArray(limelightName, \"tc\");\n    }\n\n    public static double getFiducialID(String limelightName) {\n        return getLimelightNTDouble(limelightName, \"tid\");\n    }\n\n    public static String getNeuralClassID(String limelightName) {\n        return getLimelightNTString(limelightName, \"tclass\");\n    }\n\n    public static String[] getRawBarcodeData(String limelightName) {\n        return getLimelightNTStringArray(limelightName, \"rawbarcodes\");\n    }\n\n    /////\n    /////\n\n    public static Pose3d getBotPose3d(String limelightName) {\n        double[] poseArray = getLimelightNTDoubleArray(limelightName, \"botpose\");\n        return toPose3D(poseArray);\n    }\n\n    /**\n     * (Not Recommended) Gets the robot's 3D pose in the WPILib Red Alliance Coordinate System.\n     * @param limelightName Name/identifier of the Limelight\n     * @return Pose3d object representing the robot's position and orientation in Red Alliance field space\n     */\n    public static Pose3d getBotPose3d_wpiRed(String limelightName) {\n        double[] poseArray = getLimelightNTDoubleArray(limelightName, \"botpose_wpired\");\n        return toPose3D(poseArray);\n    }\n\n    /**\n     * (Recommended) Gets the robot's 3D pose in the WPILib Blue Alliance Coordinate System.\n     * @param limelightName Name/identifier of the Limelight\n     * @return Pose3d object representing the robot's position and orientation in Blue Alliance field space\n     */\n    public static Pose3d getBotPose3d_wpiBlue(String limelightName) {\n        double[] poseArray = getLimelightNTDoubleArray(limelightName, \"botpose_wpiblue\");\n        return toPose3D(poseArray);\n    }\n\n    /**\n     * Gets the robot's 3D pose with respect to the currently tracked target's coordinate system.\n     * @param limelightName Name/identifier of the Limelight\n     * @return Pose3d object representing the robot's position and orientation relative to the target\n     */\n    public static Pose3d getBotPose3d_TargetSpace(String limelightName) {\n        double[] poseArray = getLimelightNTDoubleArray(limelightName, \"botpose_targetspace\");\n        return toPose3D(poseArray);\n    }\n\n    /**\n     * Gets the camera's 3D pose with respect to the currently tracked target's coordinate system.\n     * @param limelightName Name/identifier of the Limelight\n     * @return Pose3d object representing the camera's position and orientation relative to the target\n     */\n    public static Pose3d getCameraPose3d_TargetSpace(String limelightName) {\n        double[] poseArray = getLimelightNTDoubleArray(limelightName, \"camerapose_targetspace\");\n        return toPose3D(poseArray);\n    }\n\n    /**\n     * Gets the target's 3D pose with respect to the camera's coordinate system.\n     * @param limelightName Name/identifier of the Limelight\n     * @return Pose3d object representing the target's position and orientation relative to the camera\n     */\n    public static Pose3d getTargetPose3d_CameraSpace(String limelightName) {\n        double[] poseArray = getLimelightNTDoubleArray(limelightName, \"targetpose_cameraspace\");\n        return toPose3D(poseArray);\n    }\n\n    /**\n     * Gets the target's 3D pose with respect to the robot's coordinate system.\n     * @param limelightName Name/identifier of the Limelight\n     * @return Pose3d object representing the target's position and orientation relative to the robot\n     */\n    public static Pose3d getTargetPose3d_RobotSpace(String limelightName) {\n        double[] poseArray = getLimelightNTDoubleArray(limelightName, \"targetpose_robotspace\");\n        return toPose3D(poseArray);\n    }\n\n    /**\n     * Gets the camera's 3D pose with respect to the robot's coordinate system.\n     * @param limelightName Name/identifier of the Limelight\n     * @return Pose3d object representing the camera's position and orientation relative to the robot\n     */\n    public static Pose3d getCameraPose3d_RobotSpace(String limelightName) {\n        double[] poseArray = getLimelightNTDoubleArray(limelightName, \"camerapose_robotspace\");\n        return toPose3D(poseArray);\n    }\n\n    /**\n     * Gets the Pose2d for easy use with Odometry vision pose estimator\n     * (addVisionMeasurement)\n     * \n     * @param limelightName\n     * @return\n     */\n    public static Pose2d getBotPose2d_wpiBlue(String limelightName) {\n\n        double[] result = getBotPose_wpiBlue(limelightName);\n        return toPose2D(result);\n    }\n\n    /**\n     * Gets the MegaTag1 Pose2d and timestamp for use with WPILib pose estimator (addVisionMeasurement) in the WPILib Blue alliance coordinate system.\n     * \n     * @param limelightName\n     * @return\n     */\n    public static PoseEstimate getBotPoseEstimate_wpiBlue(String limelightName) {\n        return getBotPoseEstimate(limelightName, \"botpose_wpiblue\", false);\n    }\n\n    /**\n     * Gets the MegaTag2 Pose2d and timestamp for use with WPILib pose estimator (addVisionMeasurement) in the WPILib Blue alliance coordinate system.\n     * Make sure you are calling setRobotOrientation() before calling this method.\n     * \n     * @param limelightName\n     * @return\n     */\n    public static PoseEstimate getBotPoseEstimate_wpiBlue_MegaTag2(String limelightName) {\n        return getBotPoseEstimate(limelightName, \"botpose_orb_wpiblue\", true);\n    }\n\n    /**\n     * Gets the Pose2d for easy use with Odometry vision pose estimator\n     * (addVisionMeasurement)\n     * \n     * @param limelightName\n     * @return\n     */\n    public static Pose2d getBotPose2d_wpiRed(String limelightName) {\n\n        double[] result = getBotPose_wpiRed(limelightName);\n        return toPose2D(result);\n\n    }\n\n    /**\n     * Gets the Pose2d and timestamp for use with WPILib pose estimator (addVisionMeasurement) when you are on the RED\n     * alliance\n     * @param limelightName\n     * @return\n     */\n    public static PoseEstimate getBotPoseEstimate_wpiRed(String limelightName) {\n        return getBotPoseEstimate(limelightName, \"botpose_wpired\", false);\n    }\n\n    /**\n     * Gets the Pose2d and timestamp for use with WPILib pose estimator (addVisionMeasurement) when you are on the RED\n     * alliance\n     * @param limelightName\n     * @return\n     */\n    public static PoseEstimate getBotPoseEstimate_wpiRed_MegaTag2(String limelightName) {\n        return getBotPoseEstimate(limelightName, \"botpose_orb_wpired\", true);\n    }\n\n    /**\n     * Gets the Pose2d for easy use with Odometry vision pose estimator\n     * (addVisionMeasurement)\n     * \n     * @param limelightName\n     * @return\n     */\n    public static Pose2d getBotPose2d(String limelightName) {\n\n        double[] result = getBotPose(limelightName);\n        return toPose2D(result);\n\n    }\n   \n    /**\n     * Gets the current IMU data from NetworkTables.\n     * IMU data is formatted as [robotYaw, Roll, Pitch, Yaw, gyroX, gyroY, gyroZ, accelX, accelY, accelZ].\n     * Returns all zeros if data is invalid or unavailable.\n     * \n     * @param limelightName Name/identifier of the Limelight\n     * @return IMUData object containing all current IMU data\n     */\n    public static IMUData getIMUData(String limelightName) {\n        double[] imuData = getLimelightNTDoubleArray(limelightName, \"imu\");\n        if (imuData == null || imuData.length < 10) {\n            return new IMUData();  // Returns object with all zeros\n        }\n        return new IMUData(imuData);\n    }\n\n    /////\n    /////\n\n    public static void setPipelineIndex(String limelightName, int pipelineIndex) {\n        setLimelightNTDouble(limelightName, \"pipeline\", pipelineIndex);\n    }\n\n    \n    public static void setPriorityTagID(String limelightName, int ID) {\n        setLimelightNTDouble(limelightName, \"priorityid\", ID);\n    }\n\n    /**\n     * Sets LED mode to be controlled by the current pipeline.\n     * @param limelightName Name of the Limelight camera\n     */\n    public static void setLEDMode_PipelineControl(String limelightName) {\n        setLimelightNTDouble(limelightName, \"ledMode\", 0);\n    }\n\n    public static void setLEDMode_ForceOff(String limelightName) {\n        setLimelightNTDouble(limelightName, \"ledMode\", 1);\n    }\n\n    public static void setLEDMode_ForceBlink(String limelightName) {\n        setLimelightNTDouble(limelightName, \"ledMode\", 2);\n    }\n\n    public static void setLEDMode_ForceOn(String limelightName) {\n        setLimelightNTDouble(limelightName, \"ledMode\", 3);\n    }\n\n    /**\n     * Enables standard side-by-side stream mode.\n     * @param limelightName Name of the Limelight camera\n     */\n    public static void setStreamMode_Standard(String limelightName) {\n        setLimelightNTDouble(limelightName, \"stream\", 0);\n    }\n\n    /**\n     * Enables Picture-in-Picture mode with secondary stream in the corner.\n     * @param limelightName Name of the Limelight camera\n     */\n    public static void setStreamMode_PiPMain(String limelightName) {\n        setLimelightNTDouble(limelightName, \"stream\", 1);\n    }\n\n    /**\n     * Enables Picture-in-Picture mode with primary stream in the corner.\n     * @param limelightName Name of the Limelight camera\n     */\n    public static void setStreamMode_PiPSecondary(String limelightName) {\n        setLimelightNTDouble(limelightName, \"stream\", 2);\n    }\n\n\n\n    /**\n     * Sets the crop window for the camera. The crop window in the UI must be completely open.\n     * @param limelightName Name of the Limelight camera\n     * @param cropXMin Minimum X value (-1 to 1)\n     * @param cropXMax Maximum X value (-1 to 1)\n     * @param cropYMin Minimum Y value (-1 to 1)\n     * @param cropYMax Maximum Y value (-1 to 1)\n     */\n    public static void setCropWindow(String limelightName, double cropXMin, double cropXMax, double cropYMin, double cropYMax) {\n        double[] entries = new double[4];\n        entries[0] = cropXMin;\n        entries[1] = cropXMax;\n        entries[2] = cropYMin;\n        entries[3] = cropYMax;\n        setLimelightNTDoubleArray(limelightName, \"crop\", entries);\n    }\n   \n    /**\n     * Sets 3D offset point for easy 3D targeting.\n     */\n    public static void setFiducial3DOffset(String limelightName, double offsetX, double offsetY, double offsetZ) {\n        double[] entries = new double[3];\n        entries[0] = offsetX;\n        entries[1] = offsetY;\n        entries[2] = offsetZ;\n        setLimelightNTDoubleArray(limelightName, \"fiducial_offset_set\", entries);\n    }\n\n    /**\n     * Sets robot orientation values used by MegaTag2 localization algorithm.\n     * \n     * @param limelightName Name/identifier of the Limelight\n     * @param yaw Robot yaw in degrees. 0 = robot facing red alliance wall in FRC\n     * @param yawRate (Unnecessary) Angular velocity of robot yaw in degrees per second\n     * @param pitch (Unnecessary) Robot pitch in degrees \n     * @param pitchRate (Unnecessary) Angular velocity of robot pitch in degrees per second\n     * @param roll (Unnecessary) Robot roll in degrees\n     * @param rollRate (Unnecessary) Angular velocity of robot roll in degrees per second\n     */\n    public static void SetRobotOrientation(String limelightName, double yaw, double yawRate, \n        double pitch, double pitchRate, \n        double roll, double rollRate) {\n        SetRobotOrientation_INTERNAL(limelightName, yaw, yawRate, pitch, pitchRate, roll, rollRate, true);\n    }\n\n    public static void SetRobotOrientation_NoFlush(String limelightName, double yaw, double yawRate, \n        double pitch, double pitchRate, \n        double roll, double rollRate) {\n        SetRobotOrientation_INTERNAL(limelightName, yaw, yawRate, pitch, pitchRate, roll, rollRate, false);\n    }\n\n    private static void SetRobotOrientation_INTERNAL(String limelightName, double yaw, double yawRate, \n        double pitch, double pitchRate, \n        double roll, double rollRate, boolean flush) {\n\n        double[] entries = new double[6];\n        entries[0] = yaw;\n        entries[1] = yawRate;\n        entries[2] = pitch;\n        entries[3] = pitchRate;\n        entries[4] = roll;\n        entries[5] = rollRate;\n        setLimelightNTDoubleArray(limelightName, \"robot_orientation_set\", entries);\n        if(flush)\n        {\n            Flush();\n        }\n    }\n   \n    /**\n     * Configures the IMU mode for MegaTag2 Localization\n     * \n     * @param limelightName Name/identifier of the Limelight\n     * @param mode IMU mode.\n     */\n    public static void SetIMUMode(String limelightName, int mode) {\n        setLimelightNTDouble(limelightName, \"imumode_set\", mode);\n    }\n\n    /**\n     * Configures the complementary filter alpha value for IMU Assist Modes (Modes 3 and 4)\n     * \n     * @param limelightName Name/identifier of the Limelight\n     * @param alpha Defaults to .001. Higher values will cause the internal IMU to converge onto the assist source more rapidly.\n     */\n    public static void SetIMUAssistAlpha(String limelightName, double alpha) {\n        setLimelightNTDouble(limelightName, \"imuassistalpha_set\", alpha);\n    }\n\n    \n    /**\n     * Configures the throttle value. Set to 100-200 while disabled to reduce thermal output/temperature.\n     * \n     * @param limelightName Name/identifier of the Limelight\n     * @param throttle Defaults to 0. Your Limelgiht will process one frame after skipping <throttle> frames.\n     */\n    public static void SetThrottle(String limelightName, int throttle) {\n        setLimelightNTDouble(limelightName, \"throttle_set\", throttle);\n    }\n\n    /**\n     * Sets the 3D point-of-interest offset for the current fiducial pipeline. \n     * https://docs.limelightvision.io/docs/docs-limelight/pipeline-apriltag/apriltag-3d#point-of-interest-tracking\n     *\n     * @param limelightName Name/identifier of the Limelight\n     * @param x X offset in meters\n     * @param y Y offset in meters\n     * @param z Z offset in meters\n     */\n    public static void SetFidcuial3DOffset(String limelightName, double x, double y, \n        double z) {\n\n        double[] entries = new double[3];\n        entries[0] = x;\n        entries[1] = y;\n        entries[2] = z;\n        setLimelightNTDoubleArray(limelightName, \"fiducial_offset_set\", entries);\n    }\n\n    /**\n     * Overrides the valid AprilTag IDs that will be used for localization.\n     * Tags not in this list will be ignored for robot pose estimation.\n     *\n     * @param limelightName Name/identifier of the Limelight\n     * @param validIDs Array of valid AprilTag IDs to track\n     */\n    public static void SetFiducialIDFiltersOverride(String limelightName, int[] validIDs) {\n        double[] validIDsDouble = new double[validIDs.length];\n        for (int i = 0; i < validIDs.length; i++) {\n            validIDsDouble[i] = validIDs[i];\n        }        \n        setLimelightNTDoubleArray(limelightName, \"fiducial_id_filters_set\", validIDsDouble);\n    }\n\n    /**\n     * Sets the downscaling factor for AprilTag detection.\n     * Increasing downscale can improve performance at the cost of potentially reduced detection range.\n     * \n     * @param limelightName Name/identifier of the Limelight\n     * @param downscale Downscale factor. Valid values: 1.0 (no downscale), 1.5, 2.0, 3.0, 4.0. Set to 0 for pipeline control.\n     */\n    public static void SetFiducialDownscalingOverride(String limelightName, float downscale) \n    {\n        int d = 0; // pipeline\n        if (downscale == 1.0)\n        {\n            d = 1;\n        }\n        if (downscale == 1.5)\n        {\n            d = 2;\n        }\n        if (downscale == 2)\n        {\n            d = 3;\n        }\n        if (downscale == 3)\n        {\n            d = 4;\n        }\n        if (downscale == 4)\n        {\n            d = 5;\n        }\n        setLimelightNTDouble(limelightName, \"fiducial_downscale_set\", d);\n    }\n    \n    /**\n     * Sets the camera pose relative to the robot.\n     * @param limelightName Name of the Limelight camera\n     * @param forward Forward offset in meters\n     * @param side Side offset in meters\n     * @param up Up offset in meters\n     * @param roll Roll angle in degrees\n     * @param pitch Pitch angle in degrees\n     * @param yaw Yaw angle in degrees\n     */\n    public static void setCameraPose_RobotSpace(String limelightName, double forward, double side, double up, double roll, double pitch, double yaw) {\n        double[] entries = new double[6];\n        entries[0] = forward;\n        entries[1] = side;\n        entries[2] = up;\n        entries[3] = roll;\n        entries[4] = pitch;\n        entries[5] = yaw;\n        setLimelightNTDoubleArray(limelightName, \"camerapose_robotspace_set\", entries);\n    }\n\n    /////\n    /////\n\n    public static void setPythonScriptData(String limelightName, double[] outgoingPythonData) {\n        setLimelightNTDoubleArray(limelightName, \"llrobot\", outgoingPythonData);\n    }\n\n    public static double[] getPythonScriptData(String limelightName) {\n        return getLimelightNTDoubleArray(limelightName, \"llpython\");\n    }\n\n    /////\n    /////\n\n    /**\n     * Asynchronously take snapshot.\n     */\n    public static CompletableFuture<Boolean> takeSnapshot(String tableName, String snapshotName) {\n        return CompletableFuture.supplyAsync(() -> {\n            return SYNCH_TAKESNAPSHOT(tableName, snapshotName);\n        });\n    }\n\n    private static boolean SYNCH_TAKESNAPSHOT(String tableName, String snapshotName) {\n        URL url = getLimelightURLString(tableName, \"capturesnapshot\");\n        try {\n            HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n            connection.setRequestMethod(\"GET\");\n            if (snapshotName != null && !\"\".equals(snapshotName)) {\n                connection.setRequestProperty(\"snapname\", snapshotName);\n            }\n\n            int responseCode = connection.getResponseCode();\n            if (responseCode == 200) {\n                return true;\n            } else {\n                System.err.println(\"Bad LL Request\");\n            }\n        } catch (IOException e) {\n            System.err.println(e.getMessage());\n        }\n        return false;\n    }\n\n    /**\n     * Gets the latest JSON results output and returns a LimelightResults object.\n     * @param limelightName Name of the Limelight camera\n     * @return LimelightResults object containing all current target data\n     */\n    public static LimelightResults getLatestResults(String limelightName) {\n\n        long start = System.nanoTime();\n        LimelightHelpers.LimelightResults results = new LimelightHelpers.LimelightResults();\n        if (mapper == null) {\n            mapper = new ObjectMapper().configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);\n        }\n\n        try {\n            results = mapper.readValue(getJSONDump(limelightName), LimelightResults.class);\n        } catch (JsonProcessingException e) {\n            results.error = \"lljson error: \" + e.getMessage();\n        }\n\n        long end = System.nanoTime();\n        double millis = (end - start) * .000001;\n        results.latency_jsonParse = millis;\n        if (profileJSON) {\n            System.out.printf(\"lljson: %.2f\\r\\n\", millis);\n        }\n\n        return results;\n    }\n}"
                }
            ]
        },
        {
            "type": "link-grid",
            "title": "Resources",
            "links": [
                {
                    "label": "Vision-Based Pose Estimation with Fusion",
                    "id": "vision-pose-estimation-fusion"
                },
                {
                    "label": "WPILib AprilTag Introduction",
                    "url": "https://docs.wpilib.org/en/stable/docs/software/vision-processing/apriltag/apriltag-intro.html"
                },
                {
                    "label": "PhotonVision AprilTag Pipelines",
                    "url": "https://docs.photonvision.org/en/v2026.0.0-alpha-2/docs/apriltag-pipelines/index.html"
                },
                {
                    "label": "Limelight AprilTag Tracking",
                    "url": "https://docs.limelightvision.io/docs/docs-limelight/pipeline-apriltag/apriltags"
                },
                {
                    "label": "Limelight Robot Localization",
                    "url": "https://docs.limelightvision.io/docs/docs-limelight/pipeline-apriltag/apriltag-robot-localization"
                },
                {
                    "label": "LimelightHelpers Download",
                    "url": "https://github.com/LimelightVision/limelightlib-wpijava/blob/main/LimelightHelpers.java"
                }
            ]
        }
    ]
}