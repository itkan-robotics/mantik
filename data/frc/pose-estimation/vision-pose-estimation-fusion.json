{
    "title": "Fused Pose Estimation",
    "sections": [
        {
            "type": "text",
            "title": "What is Sensor Fusion?",
            "content": "Sensor fusion combines data from multiple sources to create a result better than any single source could provide. In FRC, we fuse <strong>Odometry</strong> (fast, smooth, but drifts) with <strong>Vision</strong> (accurate, absolute, but slower/noisier).<br><br>WPILib provides the <code>PoseEstimator</code> classes (e.g., <code>SwerveDrivePoseEstimator</code>, <code>DifferentialDrivePoseEstimator</code>) to handle this. It uses a Kalman Filter to merge the continuous odometry stream with periodic vision updates."
        },
        {
            "type": "text",
            "title": "How it Works",
            "content": "1. <strong>Predict (Odometry):</strong> Every loop (20ms), the estimator adds the distance traveled since the last loop to the previous pose. This keeps the pose update rate high for smooth control.<br>2. <strong>Correct (Vision):</strong> When a vision measurement arrives, the estimator compares it to where it <em>thought</em> the robot was. It adjusts the current pose based on the vision data, weighted by how much you \"trust\" the vision system (standard deviations)."
        },
        {
      "type": "code",
      "title": "Initializing Pose Estimator",
      "content": "package frc.robot.subsystems;\n\nimport edu.wpi.first.wpilibj2.command.SubsystemBase;\nimport edu.wpi.first.math.estimator.SwerveDrivePoseEstimator;\nimport edu.wpi.first.math.kinematics.SwerveDriveKinematics;\nimport edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.math.numbers.N3;\nimport edu.wpi.first.math.Vector;\nimport edu.wpi.first.math.VecBuilder;\nimport com.ctre.phoenix6.hardware.Pigeon2;\n\npublic class DriveSubsystem extends SubsystemBase {\n    // Standard deviations: How much we trust our measurements\n    // Low numbers = High trust\n    // (0.1, 0.1, 0.1) for State means we trust odometry a lot\n    // (0.9, 0.9, 0.9) for Vision means we trust vision less (it's noisier)\n    private static final Vector<N3> stateStdDevs = VecBuilder.fill(0.1, 0.1, 0.1);\n    private static final Vector<N3> visionStdDevs = VecBuilder.fill(0.9, 0.9, 0.9);\n\n    private SwerveDriveKinematics m_kinematics;\n    private Pigeon2 m_gyro = new Pigeon2(0);\n    private final SwerveDrivePoseEstimator m_poseEstimator;\n\n    // ... (other fields)\n\n    public DriveSubsystem() {\n        // Initialize pose estimator with kinematics, gyro, module positions, starting pose, and trust values\n        m_poseEstimator = new SwerveDrivePoseEstimator(\n            m_kinematics,\n            m_gyro.getRotation2d(),\n            getModulePositions(),\n            new Pose2d(),\n            stateStdDevs,  // Trust in odometry measurements\n            visionStdDevs  // Trust in vision measurements\n        );\n        \n        // ... (rest of constructor)\n    }\n    \n    // ... (rest of class)"
        },
        {
      "type": "code",
      "title": "Updating the Estimator",
      "content": "// ... (in DriveSubsystem class)\n\nimport java.util.Optional;\nimport org.photonvision.EstimatedRobotPose;\nimport frc.robot.subsystems.VisionSubsystem;\n\n// ... (other fields and methods)\n\n@Override\npublic void periodic() {\n    // Step 1: Update odometry every loop (20ms) with gyro and module positions\n    m_poseEstimator.update(\n        m_gyro.getRotation2d(),\n        getModulePositions()\n    );\n\n    // Step 2: Add vision measurement if available\n    Optional<EstimatedRobotPose> visionEst = m_vision.getEstimatedGlobalPose();\n    if (visionEst.isPresent()) {\n        EstimatedRobotPose camPose = visionEst.get();\n        \n        // Add vision measurement with its timestamp\n        // The estimator uses the timestamp to \"go back in time\" and apply correction correctly\n        m_poseEstimator.addVisionMeasurement(\n            camPose.estimatedPose.toPose2d(),\n            camPose.timestampSeconds\n        );\n    }\n}\n\n// ... (rest of class)"
        },
        {
            "type": "text",
            "title": "Tuning Trust (Standard Deviations)",
            "content": "The standard deviation values control the fusion mix:<br><br><strong>High Vision Std Devs (e.g., 2.0):</strong> You trust vision less. Corrections will be gradual. Good if vision is noisy.<br><strong>Low Vision Std Devs (e.g., 0.1):</strong> You trust vision highly. The pose will snap to the vision estimate. Good if vision is very stable.<br><br><strong>Dynamic Tuning:</strong> You can pass different standard deviations with each vision update. For example, trust multi-tag detections highly, but trust single-tag detections (which are prone to flipping) very little."
        },
        {
            "type": "link-grid",
            "title": "Related Topics",
            "links": [
                {
                    "label": "Odometry",
                    "id": "odometry"
                },
                {
                    "label": "Vision Subsystem",
                    "id": "vision-subsystem"
                },
                {
                    "label": "WPILib Pose Estimators",
                    "url": "https://docs.wpilib.org/en/stable/docs/software/advanced-controls/state-space/state-space-pose-estimators.html"
                }
            ]
        }
    ]
}